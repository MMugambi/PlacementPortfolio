{
  "articles": [
    {
      "path": "about.html",
      "title": "About this blog",
      "description": "Some additional details about the blog",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2025-04-18T12:13:30-04:00"
    },
    {
      "path": "index.html",
      "title": "STA-631",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2025-04-18T12:13:31-04:00"
    },
    {
      "path": "Portfolio.html",
      "title": "Portfolio",
      "author": [
        {
          "name": "Timothy Mugambi",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nResearch Question\nWhat academic, demographic, and institutional characteristics best predict whether a graduate will be successfully placed in a job after completing their degree?\n\n\nlibrary(ggformula)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(caret)\nlibrary(naniar)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(corrplot)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(themis) \nlibrary(MASS)\nlibrary(effects)\nlibrary(vip)\nlibrary(car)\n\n\n1. Introduction\nThe transition from education to employment is a pivotal moment in a graduate’s life, and understanding the factors that influence successful job placement is critical for educators, policymakers, and students alike. In the age of data-driven decision-making, leveraging predictive analytics can offer valuable insights into employment outcomes and guide strategies to enhance graduate employability.\nThis project utilizes the Job Placement Dataset(https://www.kaggle.com/datasets/mahad049/job-placement-dataset?resource=download) from Kaggle, which contains demographic, educational, and professional details of recent graduates along with their job placement status and salary (where applicable). The dataset includes key attributes such as academic performance, degree specialization, gender, and prior work experience, making it highly suitable for modeling employment outcomes.\nThe dataset is well-structured, relatively clean, and includes both categorical and numerical variables—making it an ideal candidate for demonstrating a wide range of statistical modeling techniques, including regression, classification, and dimensionality reduction. It also supports the broader social impact goal of understanding barriers to employment and highlighting equity-related patterns in job placement.\n\n\njob <- read.csv(\"job_placement.csv\")\nhead(job)\n\n  id            name gender age     degree\n1  1        John Doe   Male  25 Bachelor's\n2  2      Jane Smith Female  24 Bachelor's\n3  3 Michael Johnson   Male  26 Bachelor's\n4  4     Emily Davis Female  23 Bachelor's\n5  5     David Brown   Male  24 Bachelor's\n6  6    Sarah Wilson Female  25 Bachelor's\n                         stream                          college_name\n1              Computer Science                    Harvard University\n2        Electrical Engineering Massachusetts Institute of Technology\n3        Mechanical Engineering                   Stanford University\n4        Information Technology                       Yale University\n5              Computer Science                  Princeton University\n6 Electronics and Communication                   Columbia University\n  placement_status salary gpa years_of_experience\n1           Placed  60000 3.7                   2\n2           Placed  65000 3.6                   1\n3           Placed  58000 3.8                   3\n4       Not Placed      0 3.5                   2\n5           Placed  62000 3.9                   2\n6           Placed  63000 3.7                   1\n\nObjectives\nThis project aims to:\n1. Predict Job Placement Outcomes\nDevelop classification models to predict whether a graduate is likely to be placed based on their academic and demographic characteristics.\n2. Estimate Graduate Salary\nUse regression models to estimate the salary of placed graduates, identifying which factors most influence salary levels.\n3. Explore Nonlinear and Multiclass Outcomes\nExamine whether non-linear relationships (e.g., between academic scores and salary) exist, and use multiclass modeling to categorize salary levels (e.g., low, medium, high).\n4. Evaluate Feature Importance and Regularization Effects\nApply Lasso or Ridge regression to explore which features are most predictive of placement outcomes and how regularization can improve generalization.\nData Prepocessing\n\n\nglimpse(job)\n\nRows: 700\nColumns: 11\n$ id                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ name                <chr> \"John Doe\", \"Jane Smith\", \"Michael Johns…\n$ gender              <chr> \"Male\", \"Female\", \"Male\", \"Female\", \"Mal…\n$ age                 <int> 25, 24, 26, 23, 24, 25, 26, 24, 25, 23, …\n$ degree              <chr> \"Bachelor's\", \"Bachelor's\", \"Bachelor's\"…\n$ stream              <chr> \"Computer Science\", \"Electrical Engineer…\n$ college_name        <chr> \"Harvard University\", \"Massachusetts Ins…\n$ placement_status    <chr> \"Placed\", \"Placed\", \"Placed\", \"Not Place…\n$ salary              <int> 60000, 65000, 58000, 0, 62000, 63000, 59…\n$ gpa                 <dbl> 3.7, 3.6, 3.8, 3.5, 3.9, 3.7, 3.8, 3.6, …\n$ years_of_experience <int> 2, 1, 3, 2, 2, 1, 3, 2, 2, 1, 2, 3, 2, 1…\n\nGetting to know the values in the different columns\nKnowing the different Colleges used in the data set\n\n\nunique_values <- unique(job$college_name)\nprint(unique_values)\n\n [1] \"Harvard University\"                       \n [2] \"Massachusetts Institute of Technology\"    \n [3] \"Stanford University\"                      \n [4] \"Yale University\"                          \n [5] \"Princeton University\"                     \n [6] \"Columbia University\"                      \n [7] \"California Institute of Technology\"       \n [8] \"University of Chicago\"                    \n [9] \"University of Pennsylvania\"               \n[10] \"Northwestern University\"                  \n[11] \"Duke University\"                          \n[12] \"Johns Hopkins University\"                 \n[13] \"University of California--Berkeley\"       \n[14] \"University of Michigan--Ann Arbor\"        \n[15] \"University of California--Los Angeles\"    \n[16] \"University of Virginia\"                   \n[17] \"University of Wisconsin--Madison\"         \n[18] \"University of Illinois--Urbana-Champaign\" \n[19] \"University of North Carolina--Chapel Hill\"\n[20] \"University of Washington\"                 \n[21] \"University of California--San Diego\"      \n[22] \"University of Texas--Austin\"              \n[23] \"University of California--Santa Barbara\"  \n[24] \"University of Southern California\"        \n[25] \"University of Florida\"                    \n[26] \"University of Minnesota--Twin Cities\"     \n[27] \"University of Maryland--College Park\"     \n[28] \"University of California--Davis\"          \n[29] \"University of Pittsburgh\"                 \n[30] \"University of Colorado--Boulder\"          \n[31] \"University of Rochester\"                  \n[32] \"Boston College\"                           \n[33] \"Rice University\"                          \n[34] \"University of California--Irvine\"         \n[35] \"Georgetown University\"                    \n[36] \"University of Georgia\"                    \n[37] \"University of Notre Dame\"                 \n[38] \"University of Iowa\"                       \n[39] \"University of California--Santa Cruz\"     \n[40] \"University of Connecticut\"                \n[41] \"University of California--Riverside\"      \n[42] \"University of Delaware\"                   \n[43] \"University of California--San Francisco\"  \n[44] \"University of Texas--Dallas\"              \n\nplacement_status Column\n\n\nunique_values <- unique(job$placement_status)\nprint(unique_values)\n\n[1] \"Placed\"     \"Not Placed\"\n\nGender Column\n\n\nunique_values <- unique(job$gender)\nprint(unique_values)\n\n[1] \"Male\"   \"Female\"\n\nDegree Column\n\n\nunique_values <- unique(job$degree)\nprint(unique_values)\n\n[1] \"Bachelor's\"\n\nWe will exclude the degree column from our analysis since it exhibits no variability, containing only a single unique value across all observations, making it uninformative for modeling\nStream Column\n\n\nunique_values <- unique(job$stream)\nprint(unique_values)\n\n[1] \"Computer Science\"              \"Electrical Engineering\"       \n[3] \"Mechanical Engineering\"        \"Information Technology\"       \n[5] \"Electronics and Communication\"\n\nChecking for Missing values\n\n\nmiss_var_summary(job)\n\n# A tibble: 11 × 3\n   variable            n_miss pct_miss\n   <chr>                <int>    <num>\n 1 years_of_experience      1    0.143\n 2 id                       0    0    \n 3 name                     0    0    \n 4 gender                   0    0    \n 5 age                      0    0    \n 6 degree                   0    0    \n 7 stream                   0    0    \n 8 college_name             0    0    \n 9 placement_status         0    0    \n10 salary                   0    0    \n11 gpa                      0    0    \n\n\n\ngg_miss_var(job)\n\n\n\nFrom this we only have one missing value from years_of_experience column. The missing value has a percentage of 0.143 with less significant to the dataset if dropped, therefore we will drop the missing values from the column\n\n\njob <- job %>%\n  drop_na()\n\n\n1. GPA Distribution\n\n\nggplot(job, aes(x = gpa)) +\n  geom_histogram(binwidth = 0.1, fill = \"steelblue\", color = \"black\") +\n  labs(title = \"Distribution of GPA\", x = \"GPA\", y = \"Count\") +\n  theme_minimal()\n\n\n\nThe data shows a right-skewed distribution with GPAs ranging from approximately 3.3 to 3.9 on the standard 4.0 scale. Most students demonstrate strong academic performance, with the highest concentration occurring around the 3.7-3.75 range\n2. GPA Distribution by Placement Status\n\n\nggplot(job, aes(x = gpa, fill = placement_status)) +\n  geom_density(alpha = 0.7) +\n  labs(title = \"GPA Distribution by Placement Status\", \n       x = \"GPA\", y = \"Density\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\n\n\nThe “Not Placed” students are concentrated primarily in the lower to middle GPA range (3.4-3.7), with their highest density occurring around 3.6-3.65. “Placed” students (blue) show a multimodal distribution with three distinct peaks occurring at approximately 3.7, 3.8, and 3.9.\n3. Age Distribution\n\n\nggplot(job, aes(x = age)) +\n  geom_histogram(binwidth = 1, fill = \"#00FF7F\", color = \"black\") +\n  labs(title = \"Distribution of Age\", x = \"Age\", y = \"Count\") +\n  theme_minimal()\n\n\n\nThis data set focused on students with the age range of 23 -26.\n4. Placement Rate by Gender\n\n\ngender_placement <- job %>%\n  group_by(gender, placement_status) %>%\n  summarise(count = n(), .groups = 'drop') %>%\n  group_by(gender) %>%\n  mutate(percentage = count / sum(count) * 100)\n\nggplot(gender_placement, aes(x = gender, y = percentage, fill = placement_status)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage), \"%\")), \n            position = position_stack(vjust = 0.5)) +\n  labs(title = \"Placement Rate by Gender\", y = \"Percentage (%)\", x = \"Gender\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\n\n\nThere is a slight difference in placement outcomes between genders, with females having a marginally higher success rate (83% placed vs. 17% not placed) compared to males (80% placed vs. 20% not placed). This difference suggests that female participants may have a slight advantage in the placement process, though the difference is relatively small.\n5. Placement Rate by Stream(Course Specialization)\n\n\nstream_placement <- job %>%\n  group_by(stream, placement_status) %>%\n  summarise(count = n(), .groups = 'drop') %>%\n  group_by(stream) %>%\n  mutate(percentage = count / sum(count) * 100)\n\nggplot(stream_placement, aes(x = reorder(stream, percentage), y = percentage, \n                                fill = placement_status)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage), \"%\")), \n            position = position_stack(vjust = 0.5)) +\n  labs(title = \"Placement Rate by Stream\", y = \"Percentage (%)\", x = \"Stream\") +\n  coord_flip() +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\n\n\nElectronics and Communication has a 91% placement rate, indicating strong industry demand for graduates in this field. Information Technology has 84% placement rate, suggesting that IT skills remain highly marketable in the current job landscape.\nMechanical Engineering shows strong performance with an 81% placement rate, demonstrating that traditional engineering fields maintain solid employment prospects.\nComputer Science has the lowest placement rate among the streams at 75%, which might be surprising given the high demand for computing skills. This suggests that either there might be higher competition in this field or specific factors affecting placement for these particular Computer Science students.\nSalary Analysis of placed Student\nFor the placed students we will do a salary analysis\n6. Salary Distribution\n\n\nplaced_students <- job %>% filter(placement_status == \"Placed\")\n\nggplot(placed_students, aes(x = salary)) +\n  geom_histogram(binwidth = 2000, fill = \"purple\", color = \"black\") +\n  labs(title = \"Salary Distribution for Placed Students\", \n       x = \"Salary\", y = \"Count\") +\n  scale_x_continuous(labels = scales::comma) +\n  theme_minimal()\n\n\n\n7. Salary by Stream (Undertaken Course)\n\n\nggplot(placed_students, aes(x = stream, y = salary, fill = stream)) +\n  geom_boxplot() +\n  labs(title = \"Salary Distribution by Stream\", x = \"Stream\", y = \"Salary\") +\n  scale_y_continuous(labels = scales::comma) +\n  coord_flip() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\nComputer Science shows the highest median salary at approximately $65,000. The box spans from about $64,000 to $66,000, indicating a relatively tight interquartile range (IQR) and consistent salaries across Computer Science graduates.Electronics and Communication has the lowest median salary among these disciplines (around $63,500)\n8. GPA, Experience and Placement Status\n\n\nggplot(job, aes(x = gpa, y = years_of_experience, color = placement_status)) +\n  geom_jitter(alpha = 0.7) +\n  labs(title = \"GPA vs Experience by Placement Status\", \n       x = \"GPA\", y = \"Years of Experience\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_minimal()\n\n\n\nFrom this we can see that Experience appears to be a critical factor in placement success. Students with 3 years of experience have overwhelmingly positive placement outcomes regardless of their GPA.GPA becomes more influential for students with less experience. For those with only 1-2 years of experience, a higher GPA correlates with better placement chances.The most challenging placement scenario is for students with both low experience (1 year or less) and lower GPAs (below 3.6).\nData Preprocessing\nWe have universities but we do not have their rankings. In this stage we will create some dummy rankings(tiers) for the universities to use in our model\n\n\ntop_tier <- c(\"Harvard University\", \"Massachusetts Institute of Technology\", \n              \"Stanford University\", \"Princeton University\", \"Columbia University\",\n              \"California Institute of Technology\", \"Yale University\")\n\nsecond_tier <- c(\"University of Chicago\", \"University of Pennsylvania\", \n                \"Northwestern University\", \"Duke University\", \"Johns Hopkins University\",\n                \"University of California--Berkeley\", \"University of Michigan--Ann Arbor\",\n                \"University of California--Los Angeles\")\n\n\nCreating a binary placement for the placement_status variable.1 represents “Placed” and 0 represents any other status e.g “Not Placed”\n\n\njob <- job %>%\n  mutate(placement = ifelse(placement_status == \"Placed\", 1, 0))\nhead(job$placement)\n\n[1] 1 1 1 0 1 1\n\nCreate recipe for preprocessing\n\n\njob_recipe <- recipe(placement ~., data = job) %>%\n  #we will remove variables that will not be used in the modelling\n  step_rm(id, name, degree, placement_status) %>%\n  step_string2factor(gender, stream, college_name) %>%\n\n  step_mutate(\n    gpa_cat = case_when(\n      gpa <3.0 ~ \"low\",\n      gpa >= 3.0 & gpa < 3.5 ~ \"medium\",\n      gpa >= 3.5 & gpa < 3.8 ~ \"high\",\n      gpa >= 3.8 ~ \"very_high\"\n    )\n  )%>%\n  step_mutate(\n    college_tier = case_when(\n      college_name %in% top_tier ~ \"Tier1\",\n      college_name %in% second_tier ~ \"Tier2\",\n      TRUE ~ \"Tier3\"))%>%\n  # we now convert these ordinal variables to ordered factor\n  step_mutate(\n    gpa_cat = factor(gpa_cat, levels = c(\"low\", \"medium\", \"high\", \"very_high\"), ordered = TRUE),\n    college_tier = factor (college_tier, levels = c(\"Tier3\", \"Tier2\", \"Tier1\"), ordered = TRUE)\n  ) %>%\n  # One hot encoding for variables with fewer levels\n  step_dummy(gender, stream, one_hot = TRUE)\n\njob_prep <- prep(job_recipe, training = job)\njob_processed <- bake(job_prep, new_data = NULL)\n\nglimpse(job_processed)\n\nRows: 699\nColumns: 15\n$ age                                  <int> 25, 24, 26, 23, 24, 25,…\n$ college_name                         <fct> Harvard University, Mas…\n$ salary                               <int> 60000, 65000, 58000, 0,…\n$ gpa                                  <dbl> 3.7, 3.6, 3.8, 3.5, 3.9…\n$ years_of_experience                  <int> 2, 1, 3, 2, 2, 1, 3, 2,…\n$ placement                            <dbl> 1, 1, 1, 0, 1, 1, 1, 0,…\n$ gpa_cat                              <ord> high, high, very_high, …\n$ college_tier                         <ord> Tier1, Tier1, Tier1, Ti…\n$ gender_Female                        <dbl> 0, 1, 0, 1, 0, 1, 0, 1,…\n$ gender_Male                          <dbl> 1, 0, 1, 0, 1, 0, 1, 0,…\n$ stream_Computer.Science              <dbl> 1, 0, 0, 0, 1, 0, 0, 1,…\n$ stream_Electrical.Engineering        <dbl> 0, 1, 0, 0, 0, 0, 0, 0,…\n$ stream_Electronics.and.Communication <dbl> 0, 0, 0, 0, 0, 1, 0, 0,…\n$ stream_Information.Technology        <dbl> 0, 0, 0, 1, 0, 0, 1, 0,…\n$ stream_Mechanical.Engineering        <dbl> 0, 0, 1, 0, 0, 0, 0, 0,…\n\nCorrelation of the different variables that will be used\n\n\njob_processed %>% \n  dplyr::select(placement, age,gpa,years_of_experience, gender_Female,gender_Male) %>%\nggpairs\n\n\n\n1. Logistic Regression\n*Feature Selection\nWe will use backward selection, forward selection and stepwise method to identify the important features for the model. We’ll create a new data set for feature selection\n\n\ndata_mod <- job_processed %>%\n  dplyr:: select(-college_name) %>% # -salary\n  dplyr:: select(placement, age,gpa, years_of_experience, college_tier, starts_with(\"gender_\"), starts_with(\"stream_\"))\n\nset.seed(2025)\njob_split <- initial_split(data_mod, prop = 0.8, strata = placement)\ntrain_data <- training(job_split)\ntest_data <- testing(job_split)\n\npredictors <- c(\"age\", \"gpa\", \"years_of_experience\", \"college_tier\", \n               \"gender_Female\", \"gender_Male\",\n               \"stream_Computer.Science\", \"stream_Electrical.Engineering\", \n               \"stream_Electronics.and.Communication\", \"stream_Information.Technology\", \n               \"stream_Mechanical.Engineering\")\n\ntrain_data$placement <- as.factor(train_data$placement)\ntest_data$placement <- as.factor(test_data$placement)\n\n\nWe start with the full model\n\n\nfull_formula <- as.formula(paste(\"placement ~\", paste(predictors, collapse = \" + \")))\nfull_model <- glm(full_formula, family = binomial, data = train_data)\nsummary(full_model)\n\n\nCall:\nglm(formula = full_formula, family = binomial, data = train_data)\n\nCoefficients: (2 not defined because of singularities)\n                                      Estimate Std. Error z value\n(Intercept)                           20.59249  287.70359   0.072\nage                                   -0.01506    0.11761  -0.128\ngpa                                   -5.16650    1.61302  -3.203\nyears_of_experience                    2.65107    0.36702   7.223\ncollege_tier.L                        10.67399  610.18622   0.017\ncollege_tier.Q                         7.95213  352.29130   0.023\ngender_Female                          0.35057    0.32071   1.093\ngender_Male                                 NA         NA      NA\nstream_Computer.Science               -0.24432    0.43490  -0.562\nstream_Electrical.Engineering          0.02284    0.44605   0.051\nstream_Electronics.and.Communication   0.96826    0.52642   1.839\nstream_Information.Technology          0.55499    0.49691   1.117\nstream_Mechanical.Engineering               NA         NA      NA\n                                     Pr(>|z|)    \n(Intercept)                           0.94294    \nage                                   0.89814    \ngpa                                   0.00136 ** \nyears_of_experience                  5.08e-13 ***\ncollege_tier.L                        0.98604    \ncollege_tier.Q                        0.98199    \ngender_Female                         0.27436    \ngender_Male                                NA    \nstream_Computer.Science               0.57426    \nstream_Electrical.Engineering         0.95916    \nstream_Electronics.and.Communication  0.06587 .  \nstream_Information.Technology         0.26404    \nstream_Mechanical.Engineering              NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 537.13  on 558  degrees of freedom\nResidual deviance: 407.70  on 548  degrees of freedom\nAIC: 429.7\n\nNumber of Fisher Scoring iterations: 15\n\nCreate null model with just intercept\n\n\nnull_model <- glm(placement ~ 1, family = binomial, data = train_data)\n#summary(null_model)\n\n\nBackward Selection\n\n\ncat(\"\\n*** PERFORMING BACKWARD SELECTION ***\\n\")\n\n\n*** PERFORMING BACKWARD SELECTION ***\n\nbackward_model <- stepAIC(full_model, direction = \"backward\", \n                          trace = TRUE, k = 2)\n\nStart:  AIC=429.7\nplacement ~ age + gpa + years_of_experience + college_tier + \n    gender_Female + gender_Male + stream_Computer.Science + stream_Electrical.Engineering + \n    stream_Electronics.and.Communication + stream_Information.Technology + \n    stream_Mechanical.Engineering\n\n\nStep:  AIC=429.7\nplacement ~ age + gpa + years_of_experience + college_tier + \n    gender_Female + gender_Male + stream_Computer.Science + stream_Electrical.Engineering + \n    stream_Electronics.and.Communication + stream_Information.Technology\n\n\nStep:  AIC=429.7\nplacement ~ age + gpa + years_of_experience + college_tier + \n    gender_Female + stream_Computer.Science + stream_Electrical.Engineering + \n    stream_Electronics.and.Communication + stream_Information.Technology\n\n                                       Df Deviance    AIC\n- stream_Electrical.Engineering         1   407.70 427.70\n- age                                   1   407.71 427.71\n- stream_Computer.Science               1   408.01 428.01\n- gender_Female                         1   408.91 428.91\n- stream_Information.Technology         1   408.96 428.96\n<none>                                      407.70 429.70\n- stream_Electronics.and.Communication  1   411.25 431.25\n- gpa                                   1   418.25 438.25\n- college_tier                          2   450.90 468.90\n- years_of_experience                   1   484.13 504.13\n\nStep:  AIC=427.7\nplacement ~ age + gpa + years_of_experience + college_tier + \n    gender_Female + stream_Computer.Science + stream_Electronics.and.Communication + \n    stream_Information.Technology\n\n                                       Df Deviance    AIC\n- age                                   1   407.72 425.72\n- stream_Computer.Science               1   408.29 426.29\n- gender_Female                         1   408.94 426.94\n- stream_Information.Technology         1   409.50 427.50\n<none>                                      407.70 427.70\n- stream_Electronics.and.Communication  1   412.19 430.19\n- gpa                                   1   418.58 436.58\n- college_tier                          2   450.90 466.90\n- years_of_experience                   1   484.54 502.54\n\nStep:  AIC=425.72\nplacement ~ gpa + years_of_experience + college_tier + gender_Female + \n    stream_Computer.Science + stream_Electronics.and.Communication + \n    stream_Information.Technology\n\n                                       Df Deviance    AIC\n- stream_Computer.Science               1   408.29 424.29\n- gender_Female                         1   408.94 424.94\n- stream_Information.Technology         1   409.55 425.55\n<none>                                      407.72 425.72\n- stream_Electronics.and.Communication  1   412.19 428.19\n- gpa                                   1   418.97 434.97\n- college_tier                          2   450.90 464.90\n- years_of_experience                   1   485.52 501.52\n\nStep:  AIC=424.29\nplacement ~ gpa + years_of_experience + college_tier + gender_Female + \n    stream_Electronics.and.Communication + stream_Information.Technology\n\n                                       Df Deviance    AIC\n<none>                                      408.29 424.29\n- gender_Female                         1   411.21 425.21\n- stream_Information.Technology         1   412.83 426.83\n- stream_Electronics.and.Communication  1   414.09 428.09\n- gpa                                   1   419.88 433.88\n- college_tier                          2   451.68 463.68\n- years_of_experience                   1   487.93 501.93\n\nbackward_summary <- summary(backward_model)\n\n\nForward Selection\n\n\ncat(\"\\n*** PERFORMING FORWARD SELECTION ***\\n\")\n\n\n*** PERFORMING FORWARD SELECTION ***\n\nforward_model <- stepAIC(null_model, \n                         scope = list(lower = formula(null_model), \n                                     upper = formula(full_model)),\n                         direction = \"forward\", \n                         trace = TRUE, k = 2)\n\nStart:  AIC=539.13\nplacement ~ 1\n\n                                       Df Deviance    AIC\n+ years_of_experience                   1   476.12 480.12\n+ gpa                                   1   518.30 522.30\n+ stream_Computer.Science               1   526.42 530.42\n+ college_tier                          2   528.97 534.97\n+ stream_Electronics.and.Communication  1   531.47 535.47\n+ stream_Information.Technology         1   532.93 536.93\n<none>                                      537.13 539.13\n+ gender_Female                         1   535.39 539.39\n+ gender_Male                           1   535.39 539.39\n+ age                                   1   536.83 540.83\n+ stream_Electrical.Engineering         1   537.09 541.09\n+ stream_Mechanical.Engineering         1   537.13 541.13\n\nStep:  AIC=480.12\nplacement ~ years_of_experience\n\n                                       Df Deviance    AIC\n+ college_tier                          2   432.51 440.51\n+ stream_Computer.Science               1   464.60 470.60\n+ stream_Electronics.and.Communication  1   467.96 473.96\n+ gender_Male                           1   469.13 475.13\n+ gender_Female                         1   469.13 475.13\n+ gpa                                   1   469.62 475.62\n+ stream_Information.Technology         1   474.06 480.06\n<none>                                      476.12 480.12\n+ stream_Mechanical.Engineering         1   475.56 481.56\n+ age                                   1   475.61 481.61\n+ stream_Electrical.Engineering         1   475.79 481.79\n\nStep:  AIC=440.51\nplacement ~ years_of_experience + college_tier\n\n                                       Df Deviance    AIC\n+ gpa                                   1   421.93 431.93\n+ stream_Computer.Science               1   423.78 433.78\n+ gender_Male                           1   426.92 436.92\n+ gender_Female                         1   426.92 436.92\n+ stream_Electronics.and.Communication  1   427.45 437.45\n<none>                                      432.51 440.51\n+ stream_Information.Technology         1   431.00 441.00\n+ stream_Mechanical.Engineering         1   431.73 441.73\n+ stream_Electrical.Engineering         1   432.33 442.33\n+ age                                   1   432.38 442.38\n\nStep:  AIC=431.93\nplacement ~ years_of_experience + college_tier + gpa\n\n                                       Df Deviance    AIC\n+ stream_Computer.Science               1   414.05 426.05\n+ stream_Electronics.and.Communication  1   414.60 426.60\n+ gender_Female                         1   417.59 429.59\n+ gender_Male                           1   417.59 429.59\n<none>                                      421.93 431.93\n+ stream_Information.Technology         1   420.18 432.18\n+ age                                   1   421.24 433.24\n+ stream_Electrical.Engineering         1   421.88 433.88\n+ stream_Mechanical.Engineering         1   421.93 433.93\n\nStep:  AIC=426.05\nplacement ~ years_of_experience + college_tier + gpa + stream_Computer.Science\n\n                                       Df Deviance    AIC\n+ stream_Electronics.and.Communication  1   409.94 423.94\n<none>                                      414.05 426.05\n+ stream_Electrical.Engineering         1   412.48 426.48\n+ gender_Female                         1   412.94 426.94\n+ gender_Male                           1   412.94 426.94\n+ stream_Mechanical.Engineering         1   413.26 427.26\n+ stream_Information.Technology         1   413.95 427.95\n+ age                                   1   414.02 428.02\n\nStep:  AIC=423.94\nplacement ~ years_of_experience + college_tier + gpa + stream_Computer.Science + \n    stream_Electronics.and.Communication\n\n                                Df Deviance    AIC\n<none>                               409.94 423.94\n+ stream_Information.Technology  1   408.94 424.94\n+ stream_Electrical.Engineering  1   409.45 425.45\n+ gender_Male                    1   409.55 425.55\n+ gender_Female                  1   409.55 425.55\n+ stream_Mechanical.Engineering  1   409.82 425.82\n+ age                            1   409.91 425.91\n\nforward_summary <- summary(forward_model)\n#print(forward_summary)\n\n\nStepwise Selection\n\n\ncat(\"\\n*** PERFORMING STEPWISE SELECTION ***\\n\")\n\n\n*** PERFORMING STEPWISE SELECTION ***\n\nstepwise_model <- stepAIC(null_model, \n                          scope = list(lower = formula(null_model), \n                                      upper = formula(full_model)),\n                          direction = \"both\", \n                          trace = TRUE, k = 2)\n\nStart:  AIC=539.13\nplacement ~ 1\n\n                                       Df Deviance    AIC\n+ years_of_experience                   1   476.12 480.12\n+ gpa                                   1   518.30 522.30\n+ stream_Computer.Science               1   526.42 530.42\n+ college_tier                          2   528.97 534.97\n+ stream_Electronics.and.Communication  1   531.47 535.47\n+ stream_Information.Technology         1   532.93 536.93\n<none>                                      537.13 539.13\n+ gender_Female                         1   535.39 539.39\n+ gender_Male                           1   535.39 539.39\n+ age                                   1   536.83 540.83\n+ stream_Electrical.Engineering         1   537.09 541.09\n+ stream_Mechanical.Engineering         1   537.13 541.13\n\nStep:  AIC=480.12\nplacement ~ years_of_experience\n\n                                       Df Deviance    AIC\n+ college_tier                          2   432.51 440.51\n+ stream_Computer.Science               1   464.60 470.60\n+ stream_Electronics.and.Communication  1   467.96 473.96\n+ gender_Male                           1   469.13 475.13\n+ gender_Female                         1   469.13 475.13\n+ gpa                                   1   469.62 475.62\n+ stream_Information.Technology         1   474.06 480.06\n<none>                                      476.12 480.12\n+ stream_Mechanical.Engineering         1   475.56 481.56\n+ age                                   1   475.61 481.61\n+ stream_Electrical.Engineering         1   475.79 481.79\n- years_of_experience                   1   537.13 539.13\n\nStep:  AIC=440.51\nplacement ~ years_of_experience + college_tier\n\n                                       Df Deviance    AIC\n+ gpa                                   1   421.93 431.93\n+ stream_Computer.Science               1   423.78 433.78\n+ gender_Male                           1   426.92 436.92\n+ gender_Female                         1   426.92 436.92\n+ stream_Electronics.and.Communication  1   427.45 437.45\n<none>                                      432.51 440.51\n+ stream_Information.Technology         1   431.00 441.00\n+ stream_Mechanical.Engineering         1   431.73 441.73\n+ stream_Electrical.Engineering         1   432.33 442.33\n+ age                                   1   432.38 442.38\n- college_tier                          2   476.12 480.12\n- years_of_experience                   1   528.97 534.97\n\nStep:  AIC=431.93\nplacement ~ years_of_experience + college_tier + gpa\n\n                                       Df Deviance    AIC\n+ stream_Computer.Science               1   414.05 426.05\n+ stream_Electronics.and.Communication  1   414.60 426.60\n+ gender_Female                         1   417.59 429.59\n+ gender_Male                           1   417.59 429.59\n<none>                                      421.93 431.93\n+ stream_Information.Technology         1   420.18 432.18\n+ age                                   1   421.24 433.24\n+ stream_Electrical.Engineering         1   421.88 433.88\n+ stream_Mechanical.Engineering         1   421.93 433.93\n- gpa                                   1   432.51 440.51\n- college_tier                          2   469.62 475.62\n- years_of_experience                   1   498.09 506.09\n\nStep:  AIC=426.05\nplacement ~ years_of_experience + college_tier + gpa + stream_Computer.Science\n\n                                       Df Deviance    AIC\n+ stream_Electronics.and.Communication  1   409.94 423.94\n<none>                                      414.05 426.05\n+ stream_Electrical.Engineering         1   412.48 426.48\n+ gender_Female                         1   412.94 426.94\n+ gender_Male                           1   412.94 426.94\n+ stream_Mechanical.Engineering         1   413.26 427.26\n+ stream_Information.Technology         1   413.95 427.95\n+ age                                   1   414.02 428.02\n- stream_Computer.Science               1   421.93 431.93\n- gpa                                   1   423.78 433.78\n- college_tier                          2   458.92 466.92\n- years_of_experience                   1   487.71 497.71\n\nStep:  AIC=423.94\nplacement ~ years_of_experience + college_tier + gpa + stream_Computer.Science + \n    stream_Electronics.and.Communication\n\n                                       Df Deviance    AIC\n<none>                                      409.94 423.94\n+ stream_Information.Technology         1   408.94 424.94\n+ stream_Electrical.Engineering         1   409.45 425.45\n+ gender_Male                           1   409.55 425.55\n+ gender_Female                         1   409.55 425.55\n+ stream_Mechanical.Engineering         1   409.82 425.82\n+ age                                   1   409.91 425.91\n- stream_Electronics.and.Communication  1   414.05 426.05\n- stream_Computer.Science               1   414.60 426.60\n- gpa                                   1   421.41 433.41\n- college_tier                          2   453.58 463.58\n- years_of_experience                   1   486.80 498.80\n\nstepwise_summary <- summary(stepwise_model)\n\n\nComparing the different selection Techniques\n\n\naic_comparison <- data.frame(\n  Model = c(\"Full Model\", \"Backward Selection\", \"Forward Selection\", \"Stepwise Selection\"),\n  AIC = c(AIC(full_model), AIC(backward_model), AIC(forward_model), AIC(stepwise_model))\n)\nprint(aic_comparison)\n\n               Model      AIC\n1         Full Model 429.6968\n2 Backward Selection 424.2939\n3  Forward Selection 423.9430\n4 Stepwise Selection 423.9430\n\nThe Full Model has the highest AIC value (429.6968), suggesting it’s the least optimal approach among those tested. This likely indicates that the full model may be overfit or includes variables that don’t contribute meaningfully to predictive power.\nBackward Selection shows improvement with an AIC of 424.2939, indicating that removing certain variables from the full model improved the balance between fit and complexity.\nForward Selection and Stepwise Selection both achieved the lowest and identical AIC values (423.9430), suggesting these approaches identified the most optimal set of predictors. These methods appear to have converged on the same final model.\nThe nearly identical results between Forward and Stepwise Selection suggest that starting from different points led to the same optimal variable set, which strengthens confidence in this model specification.\nVisualizing AIC Comparison\n\n\nggplot(aic_comparison, aes(x = reorder(Model, AIC), y = AIC)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Model Comparison by AIC\",\n       x = \"Model\",\n       y = \"AIC (lower is better)\") +\n  theme_minimal()\n\n\n\nExtracting Important features\n\n\nget_model_vars <- function(model) {\n  vars <- names(coef(model))\n  vars <- vars[vars != \"(Intercept)\"]\n  return(vars)\n}\n\nbackward_vars <- get_model_vars(backward_model)\nforward_vars <- get_model_vars(forward_model)\nstepwise_vars <- get_model_vars(stepwise_model)\nfull_vars <- get_model_vars(full_model)\n\n# Create comparison of selected variables\nall_vars <- unique(c(full_vars, backward_vars, forward_vars, stepwise_vars))\n\nvar_selection <- data.frame(\n  Variable = all_vars,\n  Full = all_vars %in% full_vars,\n  Backward = all_vars %in% backward_vars,\n  Forward = all_vars %in% forward_vars,\n  Stepwise = all_vars %in% stepwise_vars\n)\n\n\nVisualizing variable\n\n\nvar_selection_long <- var_selection %>%\n  pivot_longer(cols = c(Full, Backward, Forward, Stepwise),\n               names_to = \"Model\",\n               values_to = \"Selected\") %>%\n  mutate(Selected = ifelse(Selected, \"Yes\", \"No\"))\n\nggplot(var_selection_long, aes(x = Variable, y = Model, fill = Selected)) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"white\", \"steelblue\")) +\n  labs(title = \"Feature Selection Comparison\",\n       x = \"Variable\",\n       y = \"Selection Method\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\nBoth Forward and Stepwise selection methods chose identical features, which explains the identical AIC values (423.9430). Backward Selection retained more features than Forward or stepwise selection methods. We had certain features appear in all selection methods i.e stream_Computer Science, years_of_experience, suggesting these are highly significant predictors for the model.\nModel Evaluation on Test Data\n\n\nevaluate_model <- function(model, test_data, model_name) {\n  probs <- predict(model, newdata = test_data, type = \"response\")\n  preds <- ifelse(probs > 0.5, 1, 0)\n  preds <- factor(preds, levels = levels(test_data$placement))\n  \n  conf_mat <- confusionMatrix(preds, test_data$placement)\n  \n  return(data.frame(\n    Model = model_name,\n    Accuracy = conf_mat$overall[\"Accuracy\"],\n    Sensitivity = conf_mat$byClass[\"Sensitivity\"],\n    Specificity = conf_mat$byClass[\"Specificity\"],\n    F1 = conf_mat$byClass[\"F1\"]\n  ))\n}\n\nmodel_eval <- bind_rows(\n  evaluate_model(full_model, test_data, \"Full Model\"),\n  evaluate_model(backward_model, test_data, \"Backward Selection\"),\n  evaluate_model(forward_model, test_data, \"Forward Selection\"),\n  evaluate_model(stepwise_model, test_data, \"Stepwise Selection\")\n)\n\nprint(model_eval)\n\n                          Model  Accuracy Sensitivity Specificity\nAccuracy...1         Full Model 0.7857143   0.1923077   0.9210526\nAccuracy...2 Backward Selection 0.7857143   0.1923077   0.9210526\nAccuracy...3  Forward Selection 0.7857143   0.2307692   0.9122807\nAccuracy...4 Stepwise Selection 0.7857143   0.2307692   0.9122807\n                    F1\nAccuracy...1 0.2500000\nAccuracy...2 0.2500000\nAccuracy...3 0.2857143\nAccuracy...4 0.2857143\n\nThe accuracy of the models are within the same range. I would chooose the forward selection model with an accuracy of 0.786. This model is conceptually simpler and more computationally efficient\n\n\nmodel_eval_long <- model_eval %>%\n  pivot_longer(cols = c(Accuracy, Sensitivity, Specificity, F1),\n               names_to = \"Metric\",\n               values_to = \"Value\")\n\nggplot(model_eval_long, aes(x = Model, y = Value, fill = Metric)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Performance Comparison of Feature Selection Methods\",\n       x = \"Model\",\n       y = \"Value\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  ylim(0, 1) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\ntidy(forward_model, conf.int = TRUE)\n\n# A tibble: 7 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   20.8     286.       0.0728 9.42e- 1  11.9     133.    \n2 years_of_e…    2.64      0.363    7.28   3.40e-13   1.96      3.39  \n3 college_ti…   10.6     606.       0.0175 9.86e- 1 -47.4      NA     \n4 college_ti…    7.94    350.       0.0227 9.82e- 1 -25.2      NA     \n5 gpa           -5.22      1.56    -3.34   8.41e- 4  -8.33     -2.19  \n6 stream_Com…   -0.571     0.264   -2.16   3.05e- 2  -1.09     -0.0529\n7 stream_Ele…    0.869     0.452    1.92   5.44e- 2   0.0275    1.82  \n\nYears of experience is highly significant (p-value = 3.40e-13) with a positive coefficient of 2.64, indicating that each additional year of experience increases the outcome by approximately 2.64 units, holding other variables constant. This is the most statistically significant predictor in the model.\nGPA has a significant negative relationship (p-value = 8.41e-04) with a coefficient of -5.22. This unexpected negative relationship suggests that, controlling for other factors, higher GPAs are associated with lower outcomes. Stream_Computer Science shows a significant negative effect (p-value = 0.03) with a coefficient of -0.57, suggesting this stream may have slightly worse outcomes compared to the reference stream.\nStream_Electronics and Communication is marginally significant (p-value = 0.054, just above the conventional 0.05 threshold) with a positive coefficient of 0.87, suggesting potentially better outcomes compared to the reference stream.\nCollege tier variables (college_tier_L and college_tier_Q) show very high p-values (0.986 and 0.982) and are not statistically significant predictors in this model.\nWe will drop college_tier from the model\nFinal Model\n\n\nfin_model <- glm(formula = placement ~ years_of_experience  + \n    gpa + stream_Computer.Science + stream_Electronics.and.Communication, \n    family = binomial, data = train_data)\ntidy(fin_model)\n\n# A tibble: 5 × 5\n  term                           estimate std.error statistic  p.value\n  <chr>                             <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)                      11.4       4.61       2.48 1.31e- 2\n2 years_of_experience               1.68      0.249      6.72 1.84e-11\n3 gpa                              -3.52      1.33      -2.65 8.01e- 3\n4 stream_Computer.Science          -0.631     0.252     -2.51 1.21e- 2\n5 stream_Electronics.and.Commun…    0.936     0.432      2.16 3.04e- 2\n\n\n\nplot(fin_model)\n\n\n\nFrom the residuals vs fitted values plot linearity assumption appears to be violated as there is a visible pattern in the residuals rather than random scatter around the horizontal line at zero. This non-random pattern suggests the model may be missing important predictors or transformations. Homoscedasticity assumption (constant variance of errors) also appears to be violated due to the increasing spread of residuals at higher fitted values, with larger negative residuals appearing as the fitted values increase\n2. Multiple Linear Regression (MLR)\n\n\njob_mlr <- job_processed\njob_mlr$placement_numeric <- as.numeric(job_mlr$placement) - 1\n\nset.seed(2025)\ntrain_index <- createDataPartition(job_mlr$placement_numeric, p = 0.8, list = FALSE)\ntrain_data <- job_mlr[train_index, ]\ntest_data <- job_mlr[-train_index, ]\n\nmlr_model <- lm(placement_numeric ~ age + gpa + years_of_experience + \n                college_tier + gender_Female + \n                stream_Computer.Science + stream_Electrical.Engineering +\n                stream_Electronics.and.Communication + stream_Information.Technology,\n                data = train_data)\n\ntidy(mlr_model)\n\n# A tibble: 11 × 5\n   term                          estimate std.error statistic  p.value\n   <chr>                            <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)                    -0.0666    0.735    -0.0906 9.28e- 1\n 2 age                             0.0126    0.0138    0.917  3.60e- 1\n 3 gpa                            -0.242     0.202    -1.20   2.32e- 1\n 4 years_of_experience             0.217     0.0304    7.14   2.96e-12\n 5 college_tier.L                  0.0247    0.104     0.238  8.12e- 1\n 6 college_tier.Q                  0.180     0.0666    2.71   7.01e- 3\n 7 gender_Female                  -0.0232    0.0383   -0.604  5.46e- 1\n 8 stream_Computer.Science        -0.0530    0.0541   -0.978  3.28e- 1\n 9 stream_Electrical.Engineering   0.0116    0.0553    0.211  8.33e- 1\n10 stream_Electronics.and.Commu…   0.120     0.0552    2.17   3.08e- 2\n11 stream_Information.Technology   0.0316    0.0578    0.546  5.85e- 1\n\nvif_values <- vif(mlr_model)\nprint(\"Variance Inflation Factors:\")\n\n[1] \"Variance Inflation Factors:\"\n\nprint(vif_values)\n\n                                         GVIF Df GVIF^(1/(2*Df))\nage                                  1.105069  1        1.051222\ngpa                                  2.631890  1        1.622310\nyears_of_experience                  2.525029  1        1.589034\ncollege_tier                         1.174894  2        1.041117\ngender_Female                        1.616035  1        1.271233\nstream_Computer.Science              2.810420  1        1.676431\nstream_Electrical.Engineering        1.832768  1        1.353798\nstream_Electronics.and.Communication 1.797543  1        1.340725\nstream_Information.Technology        2.404036  1        1.550495\n\nFrom the results in our Variation Infation Factor we can see that multicollinearity is not a significant concern among the predictors in the model.\nYears of experience is a strong predictor with a highly significant positive coefficient (0.217, p < 0.001), indicating that each additional year of experience substantially increases placement probability. College tier Q also shows a significant positive effect (0.180, p < 0.01), suggesting graduates from higher-tier institutions have better placement outcomes. Similarly, Electronics and Communication stream demonstrates a positive significant relationship with placement (0.120, p < 0.05). Interestingly, GPA shows a negative coefficient (-0.242) but isn’t statistically significant (p = 0.232), suggesting academic performance may not directly translate to employment success when controlling for other factors. Variables like age, gender, and most academic streams (Computer Science, Electrical Engineering) show minimal impact with non-significant coefficients\nage, gpa, gender_female, Stream_Computer.Science, Stream_Electrical.Engineering are not statistically significant to the model so we drop them\n###Final MLR Model\n\n\nmlr_model <- lm(placement_numeric ~ \n                      years_of_experience + \n                      college_tier + \n                      stream_Electronics.and.Communication,\n                      data = train_data)\ntidy(mlr_model)\n\n# A tibble: 5 × 5\n  term                           estimate std.error statistic  p.value\n  <chr>                             <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)                     -0.635     0.0667    -9.52  5.33e-20\n2 years_of_experience              0.193     0.0203     9.51  5.78e-20\n3 college_tier.L                   0.0260    0.104      0.251 8.02e- 1\n4 college_tier.Q                   0.188     0.0665     2.83  4.84e- 3\n5 stream_Electronics.and.Commun…   0.119     0.0417     2.85  4.53e- 3\n\nMaking predictions on test data and evaluating perfomance\n\n\nmlr_predictions_prob <- predict(mlr_model, newdata = test_data)\nmlr_predictions_prob <- pmin(pmax(mlr_predictions_prob, 0), 1)  \nmlr_predictions <- ifelse(mlr_predictions_prob > 0.5, 1, 0)\n\nmlr_accuracy <- mean(mlr_predictions == test_data$placement_numeric)\nprint(paste(\"MLR Accuracy:\", round(mlr_accuracy, 4)))\n\n[1] \"MLR Accuracy: 0.8129\"\n\nThis accuracy rate of 81.29% shows a strong predictive performance for job placement outcomes, indicating the reduced model with just three significant predictors (years of experience, college tier Q, and Electronics and Communication stream) effectively captures the key factors influencing employment success\nModel Diagnostics\n\n\nplot(mlr_model)\n\n\n\nThe plot shows a clear pattern in the residuals rather than random scatter around the horizontal line at zero, indicating a violation of the linearity assumption. There’s a visible downward trend from left to right, suggesting the relationship between predictors and placement might be nonlinear. The spread of residuals is inconsistent across fitted values (heteroscedasticity), with greater variance at lower fitted values and more compression at higher fitted values. This uneven spread violates the assumption of constant variance.\n3. Multinomial Regression\nResearch Question 2\nWhat factors best predict the starting salary of placed graduates?\n\n\nlibrary(nnet)\nplaced_grads <- job_processed %>% filter(placement == 1)\n\nset.seed(123)\nsalary_split <- initial_split(placed_grads, prop = 0.8)\nsalary_train <- training(salary_split)\nsalary_test <- testing(salary_split)\n\nsalary_train <- salary_train %>%\n  mutate(salary_category = case_when(\n    salary < quantile(salary, 0.33) ~ \"low\",\n    salary >= quantile(salary, 0.33) & salary < quantile(salary, 0.66) ~ \"medium\",\n    salary >= quantile(salary, 0.66) ~ \"high\"\n  ))\n\nsalary_test <- salary_test %>%\n  mutate(salary_category = case_when(\n    salary < quantile(salary_train$salary, 0.33) ~ \"low\",\n    salary >= quantile(salary_train$salary, 0.33) & salary < quantile(salary_train$salary, 0.66) ~ \"medium\",\n    salary >= quantile(salary_train$salary, 0.66) ~ \"high\"\n  ))\n\n# Convert to factor\nsalary_train$salary_category <- factor(salary_train$salary_category)\nsalary_test$salary_category <- factor(salary_test$salary_category)\n\n# Fit multinomial model with all features\nfull_model <- multinom(salary_category ~ gpa + years_of_experience + \n                         gender_Female + gender_Male + \n                         stream_Computer.Science + stream_Electrical.Engineering + \n                         stream_Electronics.and.Communication + stream_Information.Technology + \n                         stream_Mechanical.Engineering + \n                         college_tier, \n                       data = salary_train, \n                       maxit = 1000)\n\n# weights:  39 (24 variable)\ninitial  value 499.868591 \niter  10 value 376.731120\niter  20 value 346.416236\niter  30 value 338.814637\niter  40 value 338.258924\niter  50 value 338.249731\niter  50 value 338.249731\nfinal  value 338.249731 \nconverged\n\nsummary(full_model)\n\nCall:\nmultinom(formula = salary_category ~ gpa + years_of_experience + \n    gender_Female + gender_Male + stream_Computer.Science + stream_Electrical.Engineering + \n    stream_Electronics.and.Communication + stream_Information.Technology + \n    stream_Mechanical.Engineering + college_tier, data = salary_train, \n    maxit = 1000)\n\nCoefficients:\n       (Intercept)        gpa years_of_experience gender_Female\nlow       29.29696 -11.888552          -0.7827262      14.83780\nmedium   -11.85094   7.410815          -2.0222042      -5.85427\n       gender_Male stream_Computer.Science\nlow      14.459167                6.588599\nmedium   -5.996666               -1.826000\n       stream_Electrical.Engineering\nlow                         6.121840\nmedium                     -1.952113\n       stream_Electronics.and.Communication\nlow                                4.967105\nmedium                            -2.554798\n       stream_Information.Technology stream_Mechanical.Engineering\nlow                         6.134587                      5.484832\nmedium                     -2.941623                     -2.576403\n       college_tier.L college_tier.Q\nlow          9.584357       8.061926\nmedium       7.814090       5.782748\n\nStd. Errors:\n       (Intercept)      gpa years_of_experience gender_Female\nlow       6.358838 3.231986           0.3715826      3.183707\nmedium    4.523383 2.288376           0.2943991      2.248263\n       gender_Male stream_Computer.Science\nlow       3.186787                1.300543\nmedium    2.289155                0.921887\n       stream_Electrical.Engineering\nlow                        1.3106847\nmedium                     0.9551388\n       stream_Electronics.and.Communication\nlow                               1.3330029\nmedium                            0.9688091\n       stream_Information.Technology stream_Mechanical.Engineering\nlow                        1.3257838                      1.291212\nmedium                     0.9757277                      0.951369\n       college_tier.L college_tier.Q\nlow          1.609864       1.134668\nmedium       1.702709       1.107724\n\nResidual Deviance: 676.4995 \nAIC: 716.4995 \n\nMaking predictions on the test data\n\n\ntest_predictions <- predict(full_model, newdata = salary_test)\nconfusion_matrix <- confusionMatrix(test_predictions, salary_test$salary_category)\n#print(confusion_matrix)\n\n\nCalculate variable importance\n\n\n#coefficients <- coef(full_model)\n#print(coefficients)\n\nset.seed(2025)\ntrain_control <- trainControl(method = \"cv\", number = 5, classProbs = TRUE)\ncaret_model <- train(\n  salary_category ~ gpa + years_of_experience + \n    gender_Female + gender_Male + \n    stream_Computer.Science + stream_Electrical.Engineering + \n    stream_Electronics.and.Communication + stream_Information.Technology + \n    stream_Mechanical.Engineering + \n    college_tier,\n  data = salary_train,\n  method = \"multinom\",\n  trControl = train_control,\n  trace = FALSE\n)\n\nvar_imp <- varImp(caret_model)\nprint(var_imp)\n\nmultinom variable importance\n\n                                     Overall\ngender_Female                         100.00\ngender_Male                            98.67\ngpa                                    92.18\ncollege_tier.L                         38.32\ncollege_tier.Q                         36.86\nstream_Information.Technology          34.93\nstream_Computer.Science                31.20\nstream_Electrical.Engineering          29.27\nstream_Mechanical.Engineering          29.21\nstream_Electronics.and.Communication   26.14\nyears_of_experience                     0.00\n\nplot(var_imp)\n\n\n\nThe important features for this model are gender_female, gender_male, gpa, college_tier.\n\n\nfinal_model <- multinom(salary_category ~ gpa + gender_Female + gender_Male + college_tier +\n                          stream_Information.Technology,data = salary_train, maxit = 1000)\n\n# weights:  24 (14 variable)\ninitial  value 499.868591 \niter  10 value 428.058593\niter  20 value 377.164144\niter  30 value 376.046479\niter  40 value 376.045405\niter  50 value 376.038715\nfinal  value 376.037762 \nconverged\n\ntidy(final_model)\n\n# A tibble: 14 × 6\n   y.level term                  estimate std.error statistic  p.value\n   <chr>   <chr>                    <dbl>     <dbl>     <dbl>    <dbl>\n 1 low     (Intercept)            40.4        4.13      9.77  1.53e-22\n 2 low     gpa                   -15.3        1.77     -8.61  7.36e-18\n 3 low     gender_Female          20.0        2.06      9.70  2.99e-22\n 4 low     gender_Male            20.4        2.08      9.79  1.29e-22\n 5 low     college_tier.L          9.06       1.21      7.50  6.43e-14\n 6 low     college_tier.Q          7.43       0.928     8.00  1.21e-15\n 7 low     stream_Information.T…   0.0962     0.342     0.281 7.79e- 1\n 8 medium  (Intercept)             8.29       3.71      2.24  2.54e- 2\n 9 medium  gpa                    -2.46       1.57     -1.56  1.18e- 1\n10 medium  gender_Female           4.01       1.85      2.17  3.01e- 2\n11 medium  gender_Male             4.28       1.87      2.29  2.20e- 2\n12 medium  college_tier.L          7.71       1.24      6.20  5.77e-10\n13 medium  college_tier.Q          5.57       0.832     6.70  2.09e-11\n14 medium  stream_Information.T…  -0.730      0.335    -2.18  2.93e- 2\n\nModel Accuracy\n\n\nfinal_predictions <- predict(final_model, newdata = salary_test)\nfinal_confusion_matrix <- confusionMatrix(final_predictions, salary_test$salary_category)\n#print(final_confusion_matrix)\n\n# AIC comparison\nfull_model_aic <- AIC(full_model)\nfinal_model_aic <- AIC(final_model)\ncat(\"Full model AIC:\", full_model_aic, \"\\n\")\n\nFull model AIC: 716.4995 \n\ncat(\"Final model AIC:\", final_model_aic, \"\\n\")\n\nFinal model AIC: 776.0755 \n\nVisualize final model predictions on Test Data\n\n\ntest_results <- data.frame(\n  actual = salary_test$salary_category,\n  predicted = final_predictions\n)\n\ntest_probs <- predict(final_model, newdata = salary_test, type = \"probs\")\ntest_results <- cbind(test_results, test_probs)\n\ncat(\"Accuracy:\", final_confusion_matrix$overall[\"Accuracy\"], \"\\n\")\n\nAccuracy: 0.6315789 \n\nThe accuracy score of 63.16% suggests that the variables (GPA, gender, college tier, and IT specialization) capture meaningful patterns in determining salary outcomes\nIntroducing an interaction term for the Multinomial Model\nWe will introduce interaction terms to allow us to capture more complex relationships between predictors that may significantly influence salary outcomes.\n\n\nsalary_train <- salary_train %>%\n  mutate(salary_category = case_when(\n    salary < quantile(salary, 0.33) ~ \"low\",\n    salary >= quantile(salary, 0.33) & salary < quantile(salary, 0.66) ~ \"medium\",\n    salary >= quantile(salary, 0.66) ~ \"high\"\n  ))\n\nsalary_test <- salary_test %>%\n  mutate(salary_category = case_when(\n    salary < quantile(salary_train$salary, 0.33) ~ \"low\",\n    salary >= quantile(salary_train$salary, 0.33) & salary < quantile(salary_train$salary, 0.66) ~ \"medium\",\n    salary >= quantile(salary_train$salary, 0.66) ~ \"high\"\n  ))\n\n# Convert to factor\nsalary_train$salary_category <- factor(salary_train$salary_category)\nsalary_test$salary_category <- factor(salary_test$salary_category)\n\n# base model without interactions\nbase_model <- multinom(salary_category ~ gpa + gender_Female + gender_Male + college_tier +\n                          stream_Information.Technology,data = salary_train, maxit = 1000)\n\n# weights:  24 (14 variable)\ninitial  value 499.868591 \niter  10 value 428.058593\niter  20 value 377.164144\niter  30 value 376.046479\niter  40 value 376.045405\niter  50 value 376.038715\nfinal  value 376.037762 \nconverged\n\n# 1. GPA and College Tier interaction\nmodel_gpa_tier <- multinom(salary_category ~ gpa + years_of_experience + \n                             gender_Female + gender_Male + \n                             stream_Computer.Science + stream_Electrical.Engineering + \n                             stream_Electronics.and.Communication + stream_Information.Technology + \n                             stream_Mechanical.Engineering + \n                             college_tier + gpa:college_tier, \n                           data = salary_train, \n                           maxit = 1000)\n\n# weights:  45 (28 variable)\ninitial  value 499.868591 \niter  10 value 380.972959\niter  20 value 344.923340\niter  30 value 334.111672\niter  40 value 329.873546\niter  50 value 326.863548\niter  60 value 326.542543\niter  70 value 326.538306\niter  80 value 326.417576\niter  90 value 325.931121\niter 100 value 325.892026\niter 110 value 325.788115\niter 120 value 325.519059\niter 130 value 325.516123\niter 130 value 325.516120\niter 130 value 325.516119\nfinal  value 325.516119 \nconverged\n\n# 2. Experience and Stream interaction\nmodel_exp_stream <- multinom(salary_category ~ gpa + years_of_experience + \n                               gender_Female + gender_Male + \n                               stream_Computer.Science + stream_Electrical.Engineering + \n                               stream_Electronics.and.Communication + stream_Information.Technology + \n                               stream_Mechanical.Engineering + \n                               college_tier + years_of_experience:stream_Computer.Science, \n                             data = salary_train, \n                             maxit = 1000)\n\n# weights:  42 (26 variable)\ninitial  value 499.868591 \niter  10 value 388.665170\niter  20 value 349.965507\niter  30 value 338.750397\niter  40 value 338.156777\niter  50 value 338.145306\nfinal  value 338.145265 \nconverged\n\n# 3. Gender and Stream interaction\nmodel_gender_stream <- multinom(salary_category ~ gpa + years_of_experience + \n                                  gender_Female + gender_Male + \n                                  stream_Computer.Science + stream_Electrical.Engineering + \n                                  stream_Electronics.and.Communication + stream_Information.Technology + \n                                  stream_Mechanical.Engineering + \n                                  college_tier + gender_Female:stream_Computer.Science, \n                                data = salary_train, \n                                maxit = 1000)\n\n# weights:  42 (26 variable)\ninitial  value 499.868591 \niter  10 value 378.855591\niter  20 value 346.053078\niter  30 value 335.578360\niter  40 value 334.933043\niter  50 value 334.912482\nfinal  value 334.912441 \nconverged\n\n# Compare models using AIC\nmodels_aic <- data.frame(\n  Model = c(\"Base Model\", \"GPA x College Tier\", \"Experience x Stream\", \"Gender x Stream\"),\n  AIC = c(AIC(base_model), AIC(model_gpa_tier), AIC(model_exp_stream), AIC(model_gender_stream))\n)\nprint(models_aic)\n\n                Model      AIC\n1          Base Model 776.0755\n2  GPA x College Tier 699.0322\n3 Experience x Stream 720.2905\n4     Gender x Stream 713.8249\n\nTest the significance of the interaction terms using likelihood ratio tests\n\n\nlrt_gpa_tier <- anova(base_model, model_gpa_tier)\nlrt_exp_stream <- anova(base_model, model_exp_stream)\nlrt_gender_stream <- anova(base_model, model_gender_stream)\n\nprint(\"Likelihood ratio test for GPA x College Tier interaction:\")\n\n[1] \"Likelihood ratio test for GPA x College Tier interaction:\"\n\nprint(lrt_gpa_tier)\n\nLikelihood ratio tests of Multinomial Models\n\nResponse: salary_category\n                                                                                                                                                                                                                                                       Model\n1                                                                                                                                                                           gpa + gender_Female + gender_Male + college_tier + stream_Information.Technology\n2 gpa + years_of_experience + gender_Female + gender_Male + stream_Computer.Science + stream_Electrical.Engineering + stream_Electronics.and.Communication + stream_Information.Technology + stream_Mechanical.Engineering + college_tier + gpa:college_tier\n  Resid. df Resid. Dev   Test    Df LR stat.      Pr(Chi)\n1       898   752.0755                                   \n2       886   651.0322 1 vs 2    12 101.0433 3.330669e-16\n\nprint(\"Likelihood ratio test for Experience x Stream interaction:\")\n\n[1] \"Likelihood ratio test for Experience x Stream interaction:\"\n\nprint(lrt_exp_stream)\n\nLikelihood ratio tests of Multinomial Models\n\nResponse: salary_category\n                                                                                                                                                                                                                                                                                  Model\n1                                                                                                                                                                                                      gpa + gender_Female + gender_Male + college_tier + stream_Information.Technology\n2 gpa + years_of_experience + gender_Female + gender_Male + stream_Computer.Science + stream_Electrical.Engineering + stream_Electronics.and.Communication + stream_Information.Technology + stream_Mechanical.Engineering + college_tier + years_of_experience:stream_Computer.Science\n  Resid. df Resid. Dev   Test    Df LR stat.     Pr(Chi)\n1       898   752.0755                                  \n2       888   676.2905 1 vs 2    10 75.78499 3.34599e-12\n\nprint(\"Likelihood ratio test for Gender x Stream interaction:\")\n\n[1] \"Likelihood ratio test for Gender x Stream interaction:\"\n\nprint(lrt_gender_stream)\n\nLikelihood ratio tests of Multinomial Models\n\nResponse: salary_category\n                                                                                                                                                                                                                                                                            Model\n1                                                                                                                                                                                                gpa + gender_Female + gender_Male + college_tier + stream_Information.Technology\n2 gpa + years_of_experience + gender_Female + gender_Male + stream_Computer.Science + stream_Electrical.Engineering + stream_Electronics.and.Communication + stream_Information.Technology + stream_Mechanical.Engineering + college_tier + gender_Female:stream_Computer.Science\n  Resid. df Resid. Dev   Test    Df LR stat.      Pr(Chi)\n1       898   752.0755                                   \n2       888   669.8249 1 vs 2    10 82.25064 1.815215e-13\n\nEvaluate prediction accuracy for each model\nWe will now evaluate the prediction accuracy for each model based on the interaction terms used\n\n\nevaluate_model <- function(model, test_data) {\n  predictions <- predict(model, newdata = test_data)\n  conf_matrix <- confusionMatrix(predictions, test_data$salary_category)\n  return(conf_matrix$overall[\"Accuracy\"])\n}\n\nmodel_accuracies <- data.frame(\n  Model = c(\"Base Model\", \"GPA x College Tier\", \"Experience x Stream\", \"Gender x Stream\"),\n  Accuracy = c(\n    evaluate_model(base_model, salary_test),\n    evaluate_model(model_gpa_tier, salary_test),\n    evaluate_model(model_exp_stream, salary_test),\n    evaluate_model(model_gender_stream, salary_test)\n  )\n)\nprint(model_accuracies)\n\n                Model  Accuracy\n1          Base Model 0.6315789\n2  GPA x College Tier 0.7368421\n3 Experience x Stream 0.7543860\n4     Gender x Stream 0.7719298\n\nModel with all the Significant Interactions\n\n\ncomprehensive_model <- multinom(salary_category ~ gpa + gender_Female + gender_Male + stream_Computer.Science + stream_Electrical.Engineering + \n                                  stream_Electronics.and.Communication + stream_Information.Technology + stream_Mechanical.Engineering + \n                                  college_tier + years_of_experience +\n                                  gpa:college_tier + \n                                  years_of_experience:stream_Computer.Science +\n                                  gender_Female:stream_Computer.Science, data = salary_train,maxit = 1000)\n\n# weights:  51 (32 variable)\ninitial  value 499.868591 \niter  10 value 390.138364\niter  20 value 350.605069\niter  30 value 330.925238\niter  40 value 327.408011\niter  50 value 324.003214\niter  60 value 323.027960\niter  70 value 322.638388\niter  80 value 322.605682\niter  90 value 322.579467\niter 100 value 322.453946\niter 110 value 322.339336\niter 120 value 322.241995\niter 130 value 322.014129\nfinal  value 322.010431 \nconverged\n\ncomprehensive_accuracy <- evaluate_model(comprehensive_model, salary_test)\nprint(paste(\"Comprehensive model accuracy:\", comprehensive_accuracy))\n\n[1] \"Comprehensive model accuracy: 0.754385964912281\"\n\nThe incorporation of interaction terms into our multinomial logistic regression model has substantially enhanced our predictive capabilities, as seen by the improvement in accuracy from 63.16% to 75.44%. This shows that salary determination is governed by complex interrelationships\n\n\nbest_model <- comprehensive_model \ncoefficients <- coef(best_model)\nprint(coefficients)\n\n       (Intercept)        gpa gender_Female gender_Male\nlow      -10.84448   7.714602     -5.487292   -5.357191\nmedium    57.39367 -23.479701     28.800343   28.593331\n       stream_Computer.Science stream_Electrical.Engineering\nlow                  -2.123766                     -1.792237\nmedium               12.154327                     11.595757\n       stream_Electronics.and.Communication\nlow                               -2.788682\nmedium                            11.344671\n       stream_Information.Technology stream_Mechanical.Engineering\nlow                        -1.900768                      -2.23903\nmedium                     10.953021                      11.34590\n       college_tier.L college_tier.Q years_of_experience\nlow         -190.8843     -180.46375          -0.7879542\nmedium       233.6676       72.51412          -2.1715506\n       gpa:college_tier.L gpa:college_tier.Q\nlow              56.47222           51.96975\nmedium          -59.64802          -17.87235\n       stream_Computer.Science:years_of_experience\nlow                                     0.10567241\nmedium                                  0.02778847\n       gender_Female:stream_Computer.Science\nlow                                2.1835982\nmedium                             0.1987653\n\nplot_key_coefficients <- function(model) {\n  coefs <- coef(model)\n  \n  coef_data <- data.frame()\n  for (i in 1:nrow(coefs)) {\n    category <- rownames(coefs)[i]\n    temp <- data.frame(\n      Category = category,\n      Variable = colnames(coefs),\n      Coefficient = as.numeric(coefs[i,])\n    )\n    coef_data <- rbind(coef_data, temp)\n  }\n  \n  interaction_terms <- coef_data[grep(\":\", coef_data$Variable), ]\n  \n  print(\"Interaction term coefficients:\")\n  print(interaction_terms)\n  \n  return(interaction_terms)\n}\n\ninteraction_coefficients <- plot_key_coefficients(best_model)\n\n[1] \"Interaction term coefficients:\"\n   Category                                    Variable  Coefficient\n13      low                          gpa:college_tier.L  56.47221651\n14      low                          gpa:college_tier.Q  51.96975352\n15      low stream_Computer.Science:years_of_experience   0.10567241\n16      low       gender_Female:stream_Computer.Science   2.18359815\n29   medium                          gpa:college_tier.L -59.64801712\n30   medium                          gpa:college_tier.Q -17.87235269\n31   medium stream_Computer.Science:years_of_experience   0.02778847\n32   medium       gender_Female:stream_Computer.Science   0.19876532\n\n#cat(\"\\nAIC:\", AIC(best_model), \"\\n\")\ncat(\"Accuracy:\", evaluate_model(best_model, salary_test), \"\\n\")\n\nAccuracy: 0.754386 \n\n4. Linear Discriminant Analysis\nResearch Question 3\nHow does feature selection through regularization improve generalization to new graduate data for job placement?\n\n\nset.seed(2025)\njob_split <- initial_split(job_processed, prop = 0.8, strata = placement)\njob_train <- training(job_split)\njob_test <- testing(job_split)\n\njob_train$placement <- factor(job_train$placement, levels = c(0, 1), \n                            labels = c(\"Not Placed\", \"Placed\"))\njob_test$placement <- factor(job_test$placement, levels = c(0, 1), \n                           labels = c(\"Not Placed\", \"Placed\"))\n\n\nlda_formula <- as.formula(paste(\"placement ~\", paste(predictors, collapse = \" + \")))\n\nlda_model <- lda(lda_formula, data = job_train)\n\nprint(lda_model)\n\nCall:\nlda(lda_formula, data = job_train)\n\nPrior probabilities of groups:\nNot Placed     Placed \n 0.1860465  0.8139535 \n\nGroup means:\n                age      gpa years_of_experience college_tier.L\nNot Placed 24.37500 3.706731            1.653846     -0.4759373\nPlaced     24.44396 3.765055            2.309890     -0.5392661\n           college_tier.Q gender_Female gender_Male\nNot Placed    0.007850929     0.4711538   0.5288462\nPlaced        0.149840581     0.5428571   0.4571429\n           stream_Computer.Science stream_Electrical.Engineering\nNot Placed               0.4326923                     0.1730769\nPlaced                   0.2659341                     0.1648352\n           stream_Electronics.and.Communication\nNot Placed                           0.07692308\nPlaced                               0.16263736\n           stream_Information.Technology\nNot Placed                     0.1538462\nPlaced                         0.2439560\n           stream_Mechanical.Engineering\nNot Placed                     0.1634615\nPlaced                         0.1626374\n\nCoefficients of linear discriminants:\n                                             LD1\nage                                  -0.02503252\ngpa                                  -1.91910880\nyears_of_experience                   1.55342220\ncollege_tier.L                        0.92064547\ncollege_tier.Q                        1.60188179\ngender_Female                         0.09797479\ngender_Male                          -0.09797479\nstream_Computer.Science              -0.43159476\nstream_Electrical.Engineering        -0.04869535\nstream_Electronics.and.Communication  0.38308013\nstream_Information.Technology         0.33935820\nstream_Mechanical.Engineering        -0.08547302\n\ncat(\"\\nPRIOR PROBABILITIES:\\n\")\n\n\nPRIOR PROBABILITIES:\n\nprint(lda_model$prior)\n\nNot Placed     Placed \n 0.1860465  0.8139535 \n\ncat(\"\\nGROUP MEANS:\\n\")\n\n\nGROUP MEANS:\n\nprint(lda_model$means)\n\n                age      gpa years_of_experience college_tier.L\nNot Placed 24.37500 3.706731            1.653846     -0.4759373\nPlaced     24.44396 3.765055            2.309890     -0.5392661\n           college_tier.Q gender_Female gender_Male\nNot Placed    0.007850929     0.4711538   0.5288462\nPlaced        0.149840581     0.5428571   0.4571429\n           stream_Computer.Science stream_Electrical.Engineering\nNot Placed               0.4326923                     0.1730769\nPlaced                   0.2659341                     0.1648352\n           stream_Electronics.and.Communication\nNot Placed                           0.07692308\nPlaced                               0.16263736\n           stream_Information.Technology\nNot Placed                     0.1538462\nPlaced                         0.2439560\n           stream_Mechanical.Engineering\nNot Placed                     0.1634615\nPlaced                         0.1626374\n\nThe Linear Discriminant Analysis reveals several key insights into factors influencing job placement outcomes. Years of experience emerges as a critical positive predictor (coefficient 1.55), suggesting employers strongly value practical experience. Higher-tier colleges (coefficient 1.60) substantially improve placement prospects, highlighting the importance of institutional reputation in hiring decisions. Interestingly, the negative coefficient for GPA (-1.92) that contradicts conventional expectations, possibly reflecting complex interactions with other variables or specific industry preferences beyond academic performance alone. The analysis further identifies Electronics and Communication (0.38) and Information Technology (0.34) as the most advantageous specializations for placement, while Computer Science unexpectedly shows a negative association (-0.43).\nMaking Predictions\n\n\nlda_values <- predict(lda_model, job_train)\n\nlda_data <- data.frame(\n  LD1 = lda_values$x,\n  placement = job_train$placement\n)\n\nggplot(lda_data, aes(x = LD1, fill = placement)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Linear Discriminant Values by Placement Status\",\n       x = \"Linear Discriminant 1\",\n       y = \"Density\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\nlda_pred <- predict(lda_model, job_test)\ntest_pred <- lda_pred$class\ntest_probabilities <- lda_pred$posterior[, \"Placed\"]\n\npredictions_df <- data.frame(\n  actual = job_test$placement,\n  predicted = test_pred,\n  probability = test_probabilities\n)\n\n\nconf_matrix <- confusionMatrix(test_pred, job_test$placement)\n#cat(\"\\nCONFUSION MATRIX:\\n\")\n#print(conf_matrix)\n\naccuracy <- conf_matrix$overall[\"Accuracy\"]\nsensitivity <- conf_matrix$byClass[\"Sensitivity\"] \nspecificity <- conf_matrix$byClass[\"Specificity\"] \n\ncat(\"Accuracy:\", accuracy, \"\\n\")\n\nAccuracy: 0.7928571 \n\ncat(\"Sensitivity (TPR):\", sensitivity, \"\\n\")\n\nSensitivity (TPR): 0.2307692 \n\ncat(\"Specificity (TNR):\", specificity, \"\\n\")\n\nSpecificity (TNR): 0.9210526 \n\nif(require(pROC)) {\n  roc_curve <- roc(job_test$placement, test_probabilities)\n  auc_value <- auc(roc_curve)\n  \n  cat(\"AUC:\", auc_value, \"\\n\")\n  \n  plot(roc_curve, main = \"ROC Curve for LDA Model\",\n       col = \"blue\", lwd = 2)\n  abline(a = 0, b = 1, lty = 2, col = \"red\")\n}\n\nAUC: 0.759278 \n\n\nThe area under the curve estimated between 0.7-0.8, the model exhibits moderate predictive power. This model has meaningful predictive power for distinguishing between placed and not placed students, but there remains some overlap between the groups that prevents perfect classification. The model appears to be reasonably well-balanced between sensitivity and specificity, making it a useful tool for predicting placement outcomes based on the selected variables.\nFeature Importance for the model\n\n\nlda_importance <- abs(lda_model$scaling)\nlda_importance_df <- data.frame(\n  Variable = rownames(lda_importance),\n  Importance = lda_importance[,1]\n) %>%\n  arrange(desc(Importance))\n\nggplot(lda_importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Feature Importance in LDA Model\",\n       x = \"Variable\",\n       y = \"Absolute Coefficient Value\") +\n  theme_minimal()\n\n\n\nGPA is the most influential predictor with the highest absolute coefficient value (around 1.9), followed by college tier O (around 1.6) and years of experience (approximately 1.5). Age appears to be the least influential factor for placement with a negligible coefficent value.\nCompare with misclassified observations\n\n\nmisclassified <- predictions_df %>%\n  filter(actual != predicted)\n\nif(nrow(misclassified) > 0) {\n  misclassified_data <- job_test[rownames(misclassified), c(predictors, \"placement\")]\n  \n  misclassified_summary <- misclassified_data %>%\n    group_by(placement) %>%\n    summarize(\n      count = n(),\n      avg_gpa = mean(gpa, na.rm = TRUE),\n      avg_experience = mean(years_of_experience, na.rm = TRUE),\n      .groups = 'drop'\n    )\n  \n  print(misclassified_summary)\n}\n\n# A tibble: 2 × 4\n  placement  count avg_gpa avg_experience\n  <fct>      <int>   <dbl>          <dbl>\n1 Not Placed    20     3.7            1.7\n2 Placed         9     3.6            1  \n\ncompare LDA with logistic regression\n\n\nlogistic_model <- glm(lda_formula, family = binomial, data = job_train)\n\nlogistic_probs <- predict(logistic_model, newdata = job_test, type = \"response\")\nlogistic_preds <- ifelse(logistic_probs > 0.5, \"Placed\", \"Not Placed\") %>%\n  factor(levels = c(\"Not Placed\", \"Placed\"))\n\nlogistic_conf <- confusionMatrix(logistic_preds, job_test$placement)\n\nmodel_comparison <- data.frame(\n  Model = c(\"LDA\", \"Logistic Regression\"),\n  Accuracy = c(accuracy, logistic_conf$overall[\"Accuracy\"]),\n  Sensitivity = c(sensitivity, logistic_conf$byClass[\"Sensitivity\"]),\n  Specificity = c(specificity, logistic_conf$byClass[\"Specificity\"])\n)\n\nprint(model_comparison)\n\n                Model  Accuracy Sensitivity Specificity\n1                 LDA 0.7928571   0.2307692   0.9210526\n2 Logistic Regression 0.7857143   0.1923077   0.9210526\n\nThe LDA model achieves a slightly higher overall accuracy (79.3%) compared to Logistic Regression (78.6%), indicating it correctly classifies a marginally larger proportion of cases. The low sensitivity values for both models suggest they struggle significantly with identifying positive placement cases, while the high specificity indicates they’re much more reliable at identifying non-placement cases.\n5. Polynomial Regression\nResearch Question 4\nTo what extent does the relationship between academic performance (GPA) and starting salary exhibit nonlinear patterns, and how do these nonlinear effects interact with years of experience and field of study to predict graduate salary outcomes?\n\n\nn_unique_gpa <- length(unique(placed_grads$gpa))\nn_unique_exp <- length(unique(placed_grads$years_of_experience))\n\nevaluate_poly_model <- function(target_degree) {\n  max_gpa_degree <- min(n_unique_gpa - 1, target_degree)\n  max_exp_degree <- min(n_unique_exp - 1, target_degree)\n  \n  \n  max_gpa_degree <- max(1, max_gpa_degree)\n  max_exp_degree <- max(1, max_exp_degree)\n  \n  formula_str <- paste0(\"salary ~ poly(gpa, \", max_gpa_degree, \n                       \") + poly(years_of_experience, \", max_exp_degree, \n                       \") + gender_Male + stream_Computer.Science + stream_Electrical.Engineering + college_tier+ gpa_cat +\n                       stream_Electronics.and.Communication + stream_Information.Technology\")\n  \n  model <- lm(as.formula(formula_str), data = placed_grads)\n  \n\n  rmse <- sqrt(mean(model$residuals^2))\n  r_squared <- summary(model)$r.squared\n  adj_r_squared <- summary(model)$adj.r.squared\n  \n\n    return(list(\n    model = model,\n    summary = summary(model),\n    formula = formula_str,\n    metrics = data.frame(\n      gpa_degree = max_gpa_degree, \n      exp_degree = max_exp_degree,\n      rmse = rmse,\n      r_squared = r_squared,\n      adj_r_squared = adj_r_squared\n    )\n  ))\n}\n\npoly_results <- lapply(1:3, evaluate_poly_model)\n\npoly_metrics <- bind_rows(lapply(poly_results, function(x) x$metrics))\nprint(poly_metrics)\n\n  gpa_degree exp_degree     rmse r_squared adj_r_squared\n1          1          1 1785.308 0.3691478     0.3578422\n2          2          2 1638.726 0.4684872     0.4570158\n3          3          2 1636.542 0.4699031     0.4574864\n\nbest_model_idx <- which.max(poly_metrics$adj_r_squared)\nbest_poly_model <- poly_results[[best_model_idx]]$model\n\ncat(\"Best polynomial model:\\n\")\n\nBest polynomial model:\n\ncat(poly_results[[best_model_idx]]$formula, \"\\n\\n\")\n\nsalary ~ poly(gpa, 3) + poly(years_of_experience, 2) + gender_Male + stream_Computer.Science + stream_Electrical.Engineering + college_tier+ gpa_cat +\n                       stream_Electronics.and.Communication + stream_Information.Technology \n\nsummary(best_poly_model)\n\n\nCall:\nlm(formula = as.formula(formula_str), data = placed_grads)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4388.3  -576.9    17.5   980.0  4800.6 \n\nCoefficients:\n                                     Estimate Std. Error t value\n(Intercept)                          63764.87     288.88 220.735\npoly(gpa, 3)1                        44065.82    6829.73   6.452\npoly(gpa, 3)2                          430.13    2388.69   0.180\npoly(gpa, 3)3                         3993.72    3280.23   1.218\npoly(years_of_experience, 2)1        27071.23    4165.70   6.499\npoly(years_of_experience, 2)2        25024.36    2601.62   9.619\ngender_Male                            -20.29     183.89  -0.110\nstream_Computer.Science               -267.00     260.63  -1.024\nstream_Electrical.Engineering         -211.75     257.28  -0.823\ncollege_tier.L                       -2132.43     483.70  -4.409\ncollege_tier.Q                        -979.74     350.32  -2.797\ngpa_cat.L                            -2232.12     464.03  -4.810\nstream_Electronics.and.Communication   105.56     246.49   0.428\nstream_Information.Technology         -174.28     259.71  -0.671\n                                     Pr(>|t|)    \n(Intercept)                           < 2e-16 ***\npoly(gpa, 3)1                        2.41e-10 ***\npoly(gpa, 3)2                         0.85717    \npoly(gpa, 3)3                         0.22393    \npoly(years_of_experience, 2)1        1.81e-10 ***\npoly(years_of_experience, 2)2         < 2e-16 ***\ngender_Male                           0.91217    \nstream_Computer.Science               0.30607    \nstream_Electrical.Engineering         0.41085    \ncollege_tier.L                       1.25e-05 ***\ncollege_tier.Q                        0.00534 ** \ngpa_cat.L                            1.94e-06 ***\nstream_Electronics.and.Communication  0.66862    \nstream_Information.Technology         0.50247    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1657 on 555 degrees of freedom\nMultiple R-squared:  0.4699,    Adjusted R-squared:  0.4575 \nF-statistic: 37.84 on 13 and 555 DF,  p-value: < 2.2e-16\n\n# Using the degree from the best model\nbest_gpa_degree <- poly_metrics$gpa_degree[best_model_idx]\n\nggplot(placed_grads, aes(x = gpa, y = salary)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", \n              formula = paste0(\"y ~ poly(x, \", best_gpa_degree, \")\"), \n              color = \"blue\") +\n  labs(title = paste0(\"Polynomial relationship (degree \", best_gpa_degree, \") between GPA and Salary\"),\n       x = \"GPA\",\n       y = \"Salary ($)\") +\n  theme_minimal()\n\n\nbase_model <- lm(salary ~ gpa + years_of_experience + gender_Male + \n                stream_Computer.Science + stream_Electrical.Engineering + \n                stream_Electronics.and.Communication + stream_Information.Technology, \n                data = placed_grads)\n\nanova(base_model, best_poly_model)\n\nAnalysis of Variance Table\n\nModel 1: salary ~ gpa + years_of_experience + gender_Male + stream_Computer.Science + \n    stream_Electrical.Engineering + stream_Electronics.and.Communication + \n    stream_Information.Technology\nModel 2: salary ~ poly(gpa, 3) + poly(years_of_experience, 2) + gender_Male + \n    stream_Computer.Science + stream_Electrical.Engineering + \n    college_tier + gpa_cat + stream_Electronics.and.Communication + \n    stream_Information.Technology\n  Res.Df        RSS Df Sum of Sq     F    Pr(>F)    \n1    561 1906326160                                 \n2    555 1523934579  6 382391582 23.21 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe polynomial regression analysis reveals strong evidence for nonlinear relationships between academic performance and starting salary outcomes. Comparing model performance across different polynomial degrees demonstrates that a quadratic relationship (degree 2) substantially outperforms the linear model, with R-squared increasing from 0.369 to 0.468—indicating that nonlinear terms explain approximately 10% more variance in salary outcomes. This improvement is also supported by a significant reduction in prediction error, with RMSE decreasing from 1785.31 to 1638.73. The negligible improvement when advancing to a cubic model (R-squared of 0.470 versus 0.468) suggests that a quadratic function adequately captures the relationship’s nonlinearity, with the adjusted R-squared values confirming these improvements aren’t merely artifacts of additional parameters. These findings strongly indicate that the GPA-salary relationship follows a curvilinear pattern, where the marginal returns of increasing GPA may vary at different performance levels.\n\n\ntidy(best_poly_model)\n\n# A tibble: 14 × 5\n   term                          estimate std.error statistic  p.value\n   <chr>                            <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)                    63765.       289.   221.    0       \n 2 poly(gpa, 3)1                  44066.      6830.     6.45  2.41e-10\n 3 poly(gpa, 3)2                    430.      2389.     0.180 8.57e- 1\n 4 poly(gpa, 3)3                   3994.      3280.     1.22  2.24e- 1\n 5 poly(years_of_experience, 2)1  27071.      4166.     6.50  1.81e-10\n 6 poly(years_of_experience, 2)2  25024.      2602.     9.62  2.32e-20\n 7 gender_Male                      -20.3      184.    -0.110 9.12e- 1\n 8 stream_Computer.Science         -267.       261.    -1.02  3.06e- 1\n 9 stream_Electrical.Engineering   -212.       257.    -0.823 4.11e- 1\n10 college_tier.L                 -2132.       484.    -4.41  1.25e- 5\n11 college_tier.Q                  -980.       350.    -2.80  5.34e- 3\n12 gpa_cat.L                      -2232.       464.    -4.81  1.94e- 6\n13 stream_Electronics.and.Commu…    106.       246.     0.428 6.69e- 1\n14 stream_Information.Technology   -174.       260.    -0.671 5.02e- 1\n\nThe first-degree GPA term (poly(gpa, 3)1) shows a strong positive effect (44065.82, p<0.001), confirming that higher academic performance significantly increases starting salary. Interestingly, while the second-degree GPA term is not statistically significant, the third-degree term approaches significance, suggesting complex nonlinear effects at the extremes of the GPA range. Years of experience demonstrates powerful nonlinear effects, with both first-degree (27071.23, p<0.001) and second-degree (25024.36, p<0.001) terms being highly significant, indicating diminishing returns with increasing experience. Gender shows no significant salary difference (p=0.912). Academic streams (Computer Science and Electrical Engineering) show no significant differences from the reference category. However, college tier is highly significant, with Tier L institutions associated with substantially lower starting salaries (-2132.43, p<0.001) compared to the reference tier. These findings shows that salary outcomes are shaped by a complex interplay of academic performance, experience, and institutional prestige, with nonlinear effects particularly evident in the experience variable.\n\n\n\n",
      "last_modified": "2025-04-18T12:14:13-04:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
