---
title: "Portfolio"
author:
  - name: "Timothy Mugambi"
date: "`r Sys.Date()`"
output: distill::distill_article
  #rmdformats::material: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Research Question
***What academic, demographic, and institutional characteristics best predict whether a graduate will be successfully placed in a job after completing their degree?***

```{r message=FALSE}
library(ggformula)
library(tidyverse)
library(tidymodels)
library(readxl)
library(mosaic)
library(GGally)
library(caret)
library(naniar)
library(gridExtra)
library(scales)
library(corrplot)
library(recipes)
library(rsample)
library(themis) 
library(MASS)
library(effects)
library(vip)
library(car)
library(workflows)
library(tune)
library(dials)
library(yardstick)

```



## 1. Introduction
The transition from education to employment is a pivotal moment in a graduate’s life, and understanding the factors that influence successful job placement is critical for educators, policymakers, and students alike. In the age of data-driven decision-making, leveraging predictive analytics can offer valuable insights into employment outcomes and guide strategies to enhance graduate employability.

This project utilizes the Job Placement Dataset*(https://www.kaggle.com/datasets/mahad049/job-placement-dataset?resource=download)* from Kaggle, which contains demographic, educational, and professional details of recent graduates along with their job placement status and salary (where applicable). The dataset includes key attributes such as academic performance, degree specialization, gender, and prior work experience, making it highly suitable for modeling employment outcomes.

The dataset is well-structured, relatively clean, and includes both categorical and numerical variables—making it an ideal candidate for demonstrating a wide range of statistical modeling techniques, including regression, classification, and dimensionality reduction. It also supports the broader social impact goal of understanding barriers to employment and highlighting equity-related patterns in job placement.



```{r}
job <- read.csv("job_placement.csv")
head(job)
```
## Objectives
This project aims to:

**1. Predict Job Placement Outcomes**
Develop classification models to predict whether a graduate is likely to be placed based on their academic and demographic characteristics.

**2. Estimate Graduate Salary**
Use regression models to estimate the salary of placed graduates, identifying which factors most influence salary levels.

**3. Explore Nonlinear and Multiclass Outcomes**
Examine whether non-linear relationships (e.g., between academic scores and salary) exist, and use multiclass modeling to categorize salary levels (e.g., low, medium, high).

**4. Evaluate Feature Importance and Regularization Effects**
Apply Lasso or Ridge regression to explore which features are most predictive of placement outcomes and how regularization can improve generalization.


### Data Prepocessing

```{r}
glimpse(job)
```


***Getting to know the values in the different columns***
Knowing the different Colleges used in the data set
```{r}
unique_values <- unique(job$college_name)
print(unique_values)
```
**placement_status Column**
```{r}
unique_values <- unique(job$placement_status)
print(unique_values)
```



**Gender Column**
```{r}
unique_values <- unique(job$gender)
print(unique_values)
```

**Degree Column**
```{r}
unique_values <- unique(job$degree)
print(unique_values)
```

We will exclude the degree column from our analysis since it exhibits no variability, containing only a single unique value across all observations, making it uninformative for modeling

**Stream Column**
```{r}
unique_values <- unique(job$stream)
print(unique_values)
```

### Checking for Missing values
```{r}
miss_var_summary(job)
```

```{r}
gg_miss_var(job)
```
From this we only have one missing value from `years_of_experience` column. The missing value has a percentage of 0.143 with less significant to the dataset if dropped, therefore we will drop the missing values from the column
```{r}
job <- job %>%
  drop_na()
```

**1. GPA Distribution**

```{r}
ggplot(job, aes(x = gpa)) +
  geom_histogram(binwidth = 0.1, fill = "steelblue", color = "black") +
  labs(title = "Distribution of GPA", x = "GPA", y = "Count") +
  theme_minimal()
```
The data shows a right-skewed distribution with GPAs ranging from approximately 3.3 to 3.9 on the standard 4.0 scale. Most students demonstrate strong academic performance, with the highest concentration occurring around the 3.7-3.75 range

**2. GPA Distribution by Placement Status**

```{r}

ggplot(job, aes(x = gpa, fill = placement_status)) +
  geom_density(alpha = 0.7) +
  labs(title = "GPA Distribution by Placement Status", 
       x = "GPA", y = "Density") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal()

```
The "Not Placed" students are concentrated primarily in the lower to middle GPA range (3.4-3.7), with their highest density occurring around 3.6-3.65. "Placed" students (blue) show a multimodal distribution with three distinct peaks occurring at approximately 3.7, 3.8, and 3.9.

**3. Age Distribution**

```{r}
ggplot(job, aes(x = age)) +
  geom_histogram(binwidth = 1, fill = "#00FF7F", color = "black") +
  labs(title = "Distribution of Age", x = "Age", y = "Count") +
  theme_minimal()
```
This data set focused on students with the age range of 23 -26. 

**4. Placement Rate by Gender**

```{r}
gender_placement <- job %>%
  group_by(gender, placement_status) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(gender) %>%
  mutate(percentage = count / sum(count) * 100)

ggplot(gender_placement, aes(x = gender, y = percentage, fill = placement_status)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(percentage), "%")), 
            position = position_stack(vjust = 0.5)) +
  labs(title = "Placement Rate by Gender", y = "Percentage (%)", x = "Gender") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal()

```
There is a slight difference in placement outcomes between genders, with females having a marginally higher success rate (83% placed vs. 17% not placed) compared to males (80% placed vs. 20% not placed). This difference suggests that female participants may have a slight advantage in the placement process, though the difference is relatively small.

**5. Placement Rate by Stream(Course Specialization)**
```{r}
stream_placement <- job %>%
  group_by(stream, placement_status) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(stream) %>%
  mutate(percentage = count / sum(count) * 100)

ggplot(stream_placement, aes(x = reorder(stream, percentage), y = percentage, 
                                fill = placement_status)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(percentage), "%")), 
            position = position_stack(vjust = 0.5)) +
  labs(title = "Placement Rate by Stream", y = "Percentage (%)", x = "Stream") +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal()

```
Electronics and Communication has a 91% placement rate, indicating strong industry demand for graduates in this field. Information Technology has 84% placement rate, suggesting that IT skills remain highly marketable in the current job landscape.
Mechanical Engineering shows strong performance with an 81% placement rate, demonstrating that traditional engineering fields maintain solid employment prospects.
Computer Science has the lowest placement rate among the streams at 75%, which might be surprising given the high demand for computing skills. This suggests that either there might be higher competition in this field or specific factors affecting placement for these particular Computer Science students.

**Salary Analysis of placed Student**
For the placed students we will do a salary analysis
**6. Salary Distribution**

```{r}
placed_students <- job %>% filter(placement_status == "Placed")

ggplot(placed_students, aes(x = salary)) +
  geom_histogram(binwidth = 2000, fill = "purple", color = "black") +
  labs(title = "Salary Distribution for Placed Students", 
       x = "Salary", y = "Count") +
  scale_x_continuous(labels = scales::comma) +
  theme_minimal()
```
**7. Salary by Stream (Undertaken Course)**
```{r}

ggplot(placed_students, aes(x = stream, y = salary, fill = stream)) +
  geom_boxplot() +
  labs(title = "Salary Distribution by Stream", x = "Stream", y = "Salary") +
  scale_y_continuous(labels = scales::comma) +
  coord_flip() +
  theme_minimal() +
  theme(legend.position = "none")

```
Computer Science shows the highest median salary at approximately `$65,000`. The box spans from about $64,000 to $66,000, indicating a relatively tight interquartile range (IQR) and consistent salaries across Computer Science graduates.Electronics and Communication has the lowest median salary among these disciplines (around $63,500)

**8. GPA, Experience and Placement Status**


```{r}
ggplot(job, aes(x = gpa, y = years_of_experience, color = placement_status)) +
  geom_jitter(alpha = 0.7) +
  labs(title = "GPA vs Experience by Placement Status", 
       x = "GPA", y = "Years of Experience") +
  scale_color_brewer(palette = "Set1") +
  theme_minimal()

```
From this we can see that Experience appears to be a critical factor in placement success. Students with 3 years of experience have overwhelmingly positive placement outcomes regardless of their GPA.GPA becomes more influential for students with less experience. For those with only 1-2 years of experience, a higher GPA correlates with better placement chances.The most challenging placement scenario is for students with both low experience (1 year or less) and lower GPAs (below 3.6).

## Data Preprocessing 
We have universities but we do not have their rankings. In this stage we will create some dummy rankings(tiers) for the universities to use in our model

```{r}
top_tier <- c("Harvard University", "Massachusetts Institute of Technology", 
              "Stanford University", "Princeton University", "Columbia University",
              "California Institute of Technology", "Yale University")

second_tier <- c("University of Chicago", "University of Pennsylvania", 
                "Northwestern University", "Duke University", "Johns Hopkins University",
                "University of California--Berkeley", "University of Michigan--Ann Arbor",
                "University of California--Los Angeles")
```

Creating a binary placement for the `placement_status` variable.1 represents "Placed" and 0 represents any other status e.g "Not Placed"

```{r}
job <- job %>%
  mutate(placement = ifelse(placement_status == "Placed", 1, 0))
head(job$placement)
```

Create recipe for preprocessing
```{r}
job_recipe <- recipe(placement ~., data = job) %>%
  #we will remove variables that will not be used in the modelling
  step_rm(id, name, degree, placement_status) %>%
  step_string2factor(gender, stream, college_name) %>%

  step_mutate(
    gpa_cat = case_when(
      gpa <3.0 ~ "low",
      gpa >= 3.0 & gpa < 3.5 ~ "medium",
      gpa >= 3.5 & gpa < 3.8 ~ "high",
      gpa >= 3.8 ~ "very_high"
    )
  )%>%
  step_mutate(
    college_tier = case_when(
      college_name %in% top_tier ~ "Tier1",
      college_name %in% second_tier ~ "Tier2",
      TRUE ~ "Tier3"))%>%
  # we now convert these ordinal variables to ordered factor
  step_mutate(
    gpa_cat = factor(gpa_cat, levels = c("low", "medium", "high", "very_high"), ordered = TRUE),
    college_tier = factor (college_tier, levels = c("Tier3", "Tier2", "Tier1"), ordered = TRUE)
  ) %>%
  # One hot encoding for variables with fewer levels
  step_dummy(gender, stream, one_hot = TRUE)

job_prep <- prep(job_recipe, training = job)
job_processed <- bake(job_prep, new_data = NULL)

glimpse(job_processed)
```

Correlation of the different variables that will be used

```{r message=FALSE}
job_processed %>% 
  dplyr::select(placement, age,gpa,years_of_experience, gender_Female,gender_Male) %>%
ggpairs
```

## 1. Logistic Regression

***Feature Selection**
We will use backward selection, forward selection and stepwise method to identify the important features for the model. We'll create a new data set for feature selection
```{r}
data_mod <- job_processed %>%
  dplyr:: select(-college_name) %>% # -salary
  dplyr:: select(placement, age,gpa, years_of_experience, college_tier, starts_with("gender_"), starts_with("stream_"))

set.seed(2025)
job_split <- initial_split(data_mod, prop = 0.8, strata = placement)
train_data <- training(job_split)
test_data <- testing(job_split)


train_data <- train_data %>% mutate(placement = factor(placement))
test_data <- test_data %>% mutate(placement = factor(placement))

job_recipe <- recipe(placement ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

log_reg_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Create a workflow
log_reg_wf <- workflow() %>%
  add_recipe(job_recipe) %>%
  add_model(log_reg_spec)

# Fit the model
final_fit <- log_reg_wf %>%
  fit(data = train_data)

final_model <- extract_fit_parsnip(final_fit)
tidy_model <- tidy(final_model) %>%
  mutate(p_significant = p.value < 0.05) %>%
  arrange(p.value)

print("Model Coefficients:")
print(tidy_model)

test_preds <- augment(final_fit, test_data)

test_metrics <- test_preds %>%
  metrics(truth = placement, estimate = .pred_class, .pred_1)
print("Test set performance metrics:")
print(test_metrics)

```

Years of professional experience stands as the significant predictor of job placement success, with a strong positive coefficient that far outweighs other factors. Surprisingly, GPA demonstrates a negative relationship with placement outcomes, challenging conventional assumptions about academic performance as a primary hiring criterion
The various academic streams (Electronics and Communication, Information Technology, Computer Science, and Electrical Engineering), gender, and college tier did not demonstrate statistically significant relationships with the outcome.

## 2. Multiple Linear Regression (MLR)

```{r}
data_mod <- job_processed %>%
  dplyr::select(-college_name) %>% 
  dplyr::select(salary, age, gpa, years_of_experience, college_tier, 
                starts_with("gender_"), starts_with("stream_"))

set.seed(2025)
job_split <- initial_split(data_mod, prop = 0.8)
train_data <- training(job_split)
test_data <- testing(job_split)

job_recipe <- recipe(salary ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

lin_reg_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

lin_reg_wf <- workflow() %>%
  add_recipe(job_recipe) %>%
  add_model(lin_reg_spec)

final_fit <- lin_reg_wf %>%
  fit(data = train_data)

final_model <- extract_fit_parsnip(final_fit)
tidy_model <- tidy(final_model) %>%
  mutate(p_significant = p.value < 0.05) %>%
  arrange(p.value)

print("Model Coefficients:")
print(tidy_model)
print(paste("Model R-squared:", glance(final_model)$r.squared))
print(paste("Model Adjusted R-squared:", glance(final_model)$adj.r.squared))

test_preds <- augment(final_fit, test_data)

test_metrics <- test_preds %>%
  metrics(truth = salary, estimate = .pred)
print("Test set performance metrics:")
print(test_metrics)

# Visualize predictions vs actual values
prediction_plot <- ggplot(test_preds, aes(x = salary, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "blue") +
  labs(
    title = "Predicted vs Actual Salary",
    x = "Actual Salary",
    y = "Predicted Salary"
  ) +
  theme_minimal()
print(prediction_plot)

# Variable importance plot
var_importance <- tidy_model %>%
  filter(term != "(Intercept)") %>%
  mutate(
    importance = abs(estimate)
  ) %>%
  # Convert term to factor first, then reorder
  mutate(term = factor(term)) %>%
  mutate(term = forcats::fct_reorder(term, importance)) %>%
  ggplot(aes(x = importance, y = term, fill = p_significant)) +
  geom_col() +
  scale_fill_manual(values = c("grey70", "royalblue")) +
  labs(
    title = "Variable Importance in MLR Model",
    x = "Absolute Standardized Coefficient",
    y = NULL,
    fill = "Statistically\nSignificant"
  )
print(var_importance)

residual_plot <- ggplot(test_preds, aes(x = .pred, y = salary - .pred)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residual Plot",
    x = "Predicted Salary",
    y = "Residuals (Actual - Predicted)"
  ) +
  theme_minimal()
print(residual_plot)
```
The R² value of 0.194 indicates that our model explains only 19.4% of the variance in the outcome variable. This relatively low explanatory power suggests that important factors may be missing from our current model. To potentially improve model performance while managing potential multicollinearity, we will implement Ridge regression as an alternative approach.


## Ridge Regression
```{r}
library(glmnet)

ridge_spec <- linear_reg(penalty = 0.1, mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

ridge_wf <- workflow() %>%
  add_recipe(job_recipe) %>%
  add_model(ridge_spec)

ridge_fit <- ridge_wf %>%
  fit(data = train_data)

ridge_preds <- augment(ridge_fit, test_data)
ridge_metrics <- ridge_preds %>%
  metrics(truth = salary, estimate = .pred)
print(ridge_metrics)
```
Comparing our original model (R² = 0.194) with the Ridge regression results (R² = 0.162), we observe that Ridge regression actually reduced the explanatory power slightly while potentially addressing multicollinearity. The low explanatory power in both models (explaining less than 20% of variance) suggests that important factors or relationships may be missing from our current specifications. To potentially improve model performance, we will investigate interaction terms between predictors. 

## Interaction terms
These interaction effects could reveal more complex relationships that our current linear models are unable to capture, potentially leading to improved explanatory power and more nuanced insights about the factors influencing our outcome variable.

```{r}
library(forcats)

data_mod <- job_processed %>%
  dplyr::select(-college_name) %>% 
  dplyr::select(salary, age, gpa, years_of_experience, college_tier, 
                starts_with("gender_"), starts_with("stream_"))

set.seed(2025)

job_split <- initial_split(data_mod, prop = 0.8)
train_data <- training(job_split)
test_data <- testing(job_split)

# Create interaction formula components
interaction_formula <- as.formula(
  "salary ~ . + gpa:years_of_experience + age:years_of_experience + gpa:college_tier"
)

lin_reg_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

lin_reg_wf <- workflow() %>%
  add_formula(interaction_formula) %>%
  add_model(lin_reg_spec)

final_fit <- lin_reg_wf %>%
  fit(data = train_data)

final_model <- extract_fit_parsnip(final_fit)

tidy_model <- tidy(final_model) %>%
  mutate(p_significant = p.value < 0.05) %>%
  arrange(p.value)

print("Model Coefficients with Interaction Terms:")
print(tidy_model)
print(paste("Model R-squared:", glance(final_model)$r.squared))
print(paste("Model Adjusted R-squared:", glance(final_model)$adj.r.squared))

# Make predictions on test data
test_preds <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

# Calculate test metrics
test_metrics <- test_preds %>%
  metrics(truth = salary, estimate = .pred)

print("Test set performance metrics:")
print(test_metrics)

# Visualize predictions vs actual values
prediction_plot <- ggplot(test_preds, aes(x = salary, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "blue") +
  labs(
    title = "Predicted vs Actual Salary (with Interaction Terms)",
    x = "Actual Salary",
    y = "Predicted Salary"
  ) +
  theme_minimal()

print(prediction_plot)

# Variable importance plot
var_importance <- tidy_model %>%
  filter(term != "(Intercept)") %>%
  mutate(
    importance = abs(estimate)
  ) %>%
  # Convert term to factor first, then reorder
  mutate(term = factor(term)) %>%
  mutate(term = forcats::fct_reorder(term, importance)) %>%
  ggplot(aes(x = importance, y = term, fill = p_significant)) +
  geom_col() +
  scale_fill_manual(values = c("grey70", "royalblue")) +
  labs(
    title = "Variable Importance in MLR Model with Interactions",
    x = "Absolute Standardized Coefficient",
    y = NULL,
    fill = "Statistically\nSignificant"
  )

print(var_importance)

# Check residuals
residual_plot <- ggplot(test_preds, aes(x = .pred, y = salary - .pred)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residual Plot (Model with Interaction Terms)",
    x = "Predicted Salary",
    y = "Residuals (Actual - Predicted)"
  ) +
  theme_minimal()


```

The introduction of interaction terms has significantly enhanced our model's explanatory power, with the R² increasing from 19.4% in the original model to 32.8% (adjusted R² of 31.1%). This substantial improvement reveals complex relationships between predictors that weren't captured in the simpler models. The significant interaction between GPA and college tier (p < 0.0001) indicates that the impact of academic performance varies meaningfully across different institution types. Additionally, Electronics and Communication stream emerged as significant in this expanded model (p = 0.007), while years of experience maintained its significance (p = 0.02). The interaction approach uncovered nuanced relationships between educational factors and experience that the original and Ridge regression models failed to detect. These findings suggest that the effect of academic credentials on the outcome variable is not uniform but rather dependent on institutional quality and potentially moderated by professional experience, providing more sophisticated insights for decision-making in this context.


## 3. Multinomial Regression

### Research Question 2
**What factors best predict the starting salary of placed graduates?**

```{r}
placed_grads <- job_processed %>% filter(placement == 1)

set.seed(2025)
salary_split <- initial_split(placed_grads, prop = 0.8)
salary_train <- training(salary_split)
salary_test <- testing(salary_split)

train_low_threshold <- quantile(salary_train$salary, 0.33)
train_high_threshold <- quantile(salary_train$salary, 0.66)

# Create salary categories 
salary_train <- salary_train %>%
  mutate(salary_category = case_when(
    salary < train_low_threshold ~ "low",
    salary >= train_low_threshold & salary < train_high_threshold ~ "medium",
    salary >= train_high_threshold ~ "high"
  ) %>% factor())

salary_test <- salary_test %>%
  mutate(salary_category = case_when(
    salary < train_low_threshold ~ "low",
    salary >= train_low_threshold & salary < train_high_threshold ~ "medium",
    salary >= train_high_threshold ~ "high"
  ) %>% factor())

multinom_spec <- multinom_reg() %>%
  set_engine("nnet", maxit = 1000) %>%
  set_mode("classification")

full_model_fit <- multinom_spec %>%
  fit(
    salary_category ~ gpa + years_of_experience + 
      gender_Female + gender_Male + 
      stream_Computer.Science + stream_Electrical.Engineering + 
      stream_Electronics.and.Communication + stream_Information.Technology + 
      stream_Mechanical.Engineering + 
      college_tier,
    data = salary_train
  )

tidy(full_model_fit)

# Make predictions on test data
test_predictions <- full_model_fit %>%
  predict(new_data = salary_test) %>%
  pull(.pred_class)



# Cross-validation 
set.seed(2025)
salary_folds <- vfold_cv(salary_train, v = 5)

cv_results <- fit_resamples(
  multinom_spec,
  salary_category ~ gpa + years_of_experience + 
    gender_Female + gender_Male + 
    stream_Computer.Science + stream_Electrical.Engineering + 
    stream_Electronics.and.Communication + stream_Information.Technology + 
    stream_Mechanical.Engineering + 
    college_tier,
  resamples = salary_folds,
  metrics = metric_set(accuracy, roc_auc),
  control = control_resamples(save_pred = TRUE)
)

collect_metrics(cv_results)



final_model_fit <- multinom_spec %>%
  fit(
    salary_category ~ gpa + years_of_experience + college_tier,
    data = salary_train
  )

final_predictions <- final_model_fit %>%
  predict(new_data = salary_test) %>%
  pull(.pred_class)

```
GPA has a significant negative association with being in the low salary group (p=0.002), suggesting that higher GPAs reduce the likelihood of receiving a lower starting salary. Both gender categories showed strong positive effects, indicating certain gender-related patterns in salary distribution. All engineering streams demonstrated significant positive coefficients, with Electrical Engineering having the strongest impact (p<0.0001). Years of experience was not statistically significant for the low salary group (p=0.219). The model's performance metrics with an accuracy (66.2%) but a strong ROC AUC (0.802), indicating good discriminative ability between salary categories. 

# Introducing an interaction term for the Multinomial Model
We will introduce interaction terms to allow us to capture more complex relationships between predictors that may significantly influence salary outcomes. 

```{r}
# Adding interaction terms to the full model
interaction_model_fit <- multinom_spec %>%
  fit(
    salary_category ~ gpa + years_of_experience + 
      gender_Female + gender_Male + 
      stream_Computer.Science + stream_Electrical.Engineering + 
      stream_Electronics.and.Communication + stream_Information.Technology + 
      stream_Mechanical.Engineering + 
      college_tier +
      gpa:years_of_experience +                  
      gpa:college_tier +                        
      years_of_experience:college_tier +        
      gender_Female:stream_Computer.Science +   
      gender_Male:stream_Computer.Science,      
    data = salary_train
  )

tidy(interaction_model_fit)

interaction_predictions <- interaction_model_fit %>%
  predict(new_data = salary_test) %>%
  pull(.pred_class)


# Cross-validation for the interaction model
cv_results_interaction <- fit_resamples(
  multinom_spec,
  salary_category ~ gpa + years_of_experience + 
    gender_Female + gender_Male + 
    stream_Computer.Science + stream_Electrical.Engineering + 
    stream_Electronics.and.Communication + stream_Information.Technology + 
    stream_Mechanical.Engineering + 
    college_tier +
    gpa:years_of_experience +
    gpa:college_tier +
    years_of_experience:college_tier,
  resamples = salary_folds,
  metrics = metric_set(accuracy, roc_auc),
  control = control_resamples(save_pred = TRUE)
)

# Compare CV metrics
collect_metrics(cv_results) %>% 
  mutate(model = "Without Interactions") %>%
  bind_rows(
    collect_metrics(cv_results_interaction) %>% 
      mutate(model = "With Interactions")
  ) %>%
  arrange(.metric, model)
```
For accuracy, the model with interactions (65.9%) performs marginally worse than the model without interactions (66.2%), this shows that interaction terms don't meaningfully improve the model's predictive capability for classifying salary categories.

## Comparing Lasso and Ridge Regression 
we will use lasso and ridge regression to check if the perfomance of the model can improve

```{r}
set.seed(2025)

salary_recipe <- recipe(
  salary_category ~ gpa + years_of_experience + 
    gender_Female + gender_Male + 
    stream_Computer.Science + stream_Electrical.Engineering + 
    stream_Electronics.and.Communication + stream_Information.Technology + 
    stream_Mechanical.Engineering + 
    college_tier,
  data = salary_train
) %>%
  # Convert ordered factor to numeric
  step_ordinalscore(college_tier) %>%
  # Normalize all predictors (important for regularization)
  step_normalize(all_predictors())

penalty_grid <- grid_regular(
  penalty(range = c(-3, 0), trans = log10_trans()),
  levels = 10
)

# Ridge model specification
ridge_spec <- multinom_reg(
  penalty = tune(),
  mixture = 0  # Ridge regression (L2 penalty)
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Lasso model specification
lasso_spec <- multinom_reg(
  penalty = tune(),
  mixture = 1  # Lasso regression (L1 penalty)
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

ridge_wf <- workflow() %>%
  add_recipe(salary_recipe) %>%
  add_model(ridge_spec)

lasso_wf <- workflow() %>%
  add_recipe(salary_recipe) %>%
  add_model(lasso_spec)

# Set up cross-validation
set.seed(2025)
salary_folds <- vfold_cv(salary_train, v = 5)

# Tune ridge model
ridge_results <- tune_grid(
  ridge_wf,
  resamples = salary_folds,
  grid = penalty_grid,
  metrics = metric_set(accuracy, roc_auc)
)

# Tune lasso model
lasso_results <- tune_grid(
  lasso_wf,
  resamples = salary_folds,
  grid = penalty_grid,
  metrics = metric_set(accuracy, roc_auc)
)

ridge_metrics <- collect_metrics(ridge_results) %>%
  mutate(model = "Ridge")

lasso_metrics <- collect_metrics(lasso_results) %>%
  mutate(model = "Lasso")

combined_metrics <- bind_rows(ridge_metrics, lasso_metrics)

# Select best hyperparameters
best_ridge <- select_best(ridge_results, metric = "accuracy")
best_lasso <- select_best(lasso_results, metric = "accuracy")

final_ridge_wf <- ridge_wf %>%
  finalize_workflow(best_ridge)

final_lasso_wf <- lasso_wf %>%
  finalize_workflow(best_lasso)

# Fit final models
final_ridge_fit <- final_ridge_wf %>%
  fit(data = salary_train)

final_lasso_fit <- final_lasso_wf %>%
  fit(data = salary_train)

# Make predictions
ridge_predictions <- final_ridge_fit %>%
  predict(new_data = salary_test) %>%
  pull(.pred_class)

lasso_predictions <- final_lasso_fit %>%
  predict(new_data = salary_test) %>%
  pull(.pred_class)

ridge_acc <- accuracy(
  data = data.frame(truth = salary_test$salary_category, estimate = ridge_predictions),
  truth = truth,
  estimate = estimate
)

lasso_acc <- accuracy(
  data = data.frame(truth = salary_test$salary_category, estimate = lasso_predictions),
  truth = truth,
  estimate = estimate
)

# compare the models
  model_comparison <- bind_rows(
  ridge_acc %>% mutate(model = "Ridge"),
  lasso_acc %>% mutate(model = "Lasso"))

  model_comparison %>%
  pivot_wider(names_from = model, values_from = .estimate) %>%
  arrange(.metric)

# Extract and compare coefficients
ridge_coefs <- tidy(final_ridge_fit) %>%
  mutate(model = "Ridge")

lasso_coefs <- tidy(final_lasso_fit) %>%
  mutate(model = "Lasso")

all_coefs <- bind_rows(ridge_coefs, lasso_coefs)

# Visualize coefficients for one outcome level
ggplot(all_coefs %>% filter(class == "high"), 
       aes(x = term, y = estimate, fill = model)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Coefficient Comparison for 'High' Salary Category",
       x = "Predictors",
       y = "Coefficient Estimates") +
  theme_minimal()

# Variable importance for both models
vip_ridge <- extract_fit_engine(final_ridge_fit) %>%
  vi(lambda = best_ridge$penalty) %>%
  mutate(model = "Ridge")

vip_lasso <- extract_fit_engine(final_lasso_fit) %>%
  vi(lambda = best_lasso$penalty) %>%
  mutate(model = "Lasso")

# Compare variable importance
bind_rows(vip_ridge, vip_lasso) %>%
  group_by(model) %>%
  top_n(10, Importance) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(Variable, Importance), y = Importance, fill = model)) +
  geom_col(position = "dodge") +
  coord_flip() +
  facet_wrap(~model, scales = "free_y") +
  labs(title = "Top 10 Variables by Importance",
       x = NULL) +
  theme_minimal()

```
The results from Ridge and Lasso regularization demonstrate remarkable consistency and improvement in model performance, with both techniques achieving identical accuracy scores of 72.8%. This represents a substantial increase from the previous models' accuracy of approximately 66%, indicating that regularization effectively addressed potential overfitting issues in the original models. The identical performance between Ridge and Lasso suggests that both approaches identified similar patterns in the data and applied comparable levels of feature penalization.
This can be confirmed from the variable importance plots from both Lasso and Ridge regularization models that reveal a consistent pattern in the factors that best predict graduate starting salaries.

# 4. Linear Discriminant Analysis

## Research Question 3
**What factors contribute most to the classification of graduate job placement outcomes using Linear Discriminant Analysis?**
```{r}
data_mod <- job_processed %>%
  dplyr::select(-college_name) %>%
  dplyr::select(placement, age, gpa, years_of_experience, college_tier, 
                starts_with("gender_"), starts_with("stream_"))

set.seed(2025)
job_split <- initial_split(data_mod, prop = 0.8, strata = placement)
train_data <- training(job_split)
test_data <- testing(job_split)


if(!is.factor(train_data$placement)) {
  train_data <- train_data %>%
    mutate(placement = factor(placement, 
                             levels = c(0, 1),
                             labels = c("Not Placed", "Placed")))
  
  test_data <- test_data %>%
    mutate(placement = factor(placement, 
                            levels = c(0, 1),
                            labels = c("Not Placed", "Placed")))
}

# Create formula for LDA
predictors <- setdiff(names(train_data), "placement")
lda_formula <- as.formula(paste("placement ~", paste(predictors, collapse = " + ")))

lda_model <- lda(lda_formula, data = train_data)

train_predictions <- predict(lda_model, train_data)

lda_data <- tibble(
  LD1 = train_predictions$x,
  placement = train_data$placement
)
```
**Make Predictions on test data and plotting density status**

```{r}
ggplot(lda_data, aes(x = LD1, fill = placement)) +
  geom_density(alpha = 0.5) +
  labs(title = "Linear Discriminant Values by Placement Status",
       x = "Linear Discriminant 1",
       y = "Density") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")

test_predictions <- predict(lda_model, test_data)

posterior_colnames <- colnames(test_predictions$posterior)
placement_col <- posterior_colnames[length(posterior_colnames)] 

predictions_df <- tibble(
  actual = test_data$placement,
  predicted = test_predictions$class,
  probability = test_predictions$posterior[, placement_col]
)

conf_matrix <- confusionMatrix(predictions_df$predicted, predictions_df$actual)

cat("Accuracy:", conf_matrix$overall["Accuracy"], "\n")
cat("Sensitivity (TPR):", conf_matrix$byClass["Sensitivity"], "\n")
cat("Specificity (TNR):", conf_matrix$byClass["Specificity"], "\n")


misclassified <- predictions_df %>%
  filter(actual != predicted)

if(nrow(misclassified) > 0) {
  # Get indices of misclassified observations
  misclassified_indices <- which(predictions_df$actual != predictions_df$predicted)
  
  # Extract misclassified data
  misclassified_data <- test_data[misclassified_indices, ]
  
  # Summarize misclassified observations
  misclassified_summary <- misclassified_data %>%
    group_by(placement) %>%
    summarize(
      count = n(),
      avg_gpa = mean(gpa, na.rm = TRUE),
      avg_experience = mean(years_of_experience, na.rm = TRUE),
      .groups = 'drop'
    )
  
  print(misclassified_summary)
}
```

The linear discriminant analysis model achieved a respectable accuracy of 79.3%, demonstrating its effectiveness in distinguishing between placed and non-placed graduates
The density plot reveals clear separation between the two groups, with not-placed students concentrated on the negative side of the discriminant axis and placed students primarily on the positive side



# 5. Polynomial Regression

## Research Question 4
**To what extent does the relationship between academic performance (GPA) and starting salary exhibit nonlinear patterns, and how do these nonlinear effects interact with years of experience and field of study to predict graduate salary outcomes?**

```{r}
evaluate_poly_model_tidy <- function(target_degree, data = placed_grads) {
  # Calculate maximum polynomial degrees based on unique values
  n_unique_gpa <- length(unique(data$gpa))
  n_unique_exp <- length(unique(data$years_of_experience))
  
  max_gpa_degree <- min(n_unique_gpa - 1, target_degree)
  max_exp_degree <- min(n_unique_exp - 1, target_degree)
  
  max_gpa_degree <- max(1, max_gpa_degree)
  max_exp_degree <- max(1, max_exp_degree)
  
  model_recipe <- recipe(salary ~ gpa + years_of_experience + gender_Male + 
                         stream_Computer.Science + stream_Electrical.Engineering + 
                         college_tier + gpa_cat + stream_Electronics.and.Communication + 
                         stream_Information.Technology, 
                         data = data) %>%
    step_poly(gpa, degree = max_gpa_degree) %>%
    step_poly(years_of_experience, degree = max_exp_degree)
  
  lin_reg_spec <- linear_reg() %>%
    set_engine("lm") %>%
    set_mode("regression")
  
  # Create workflow combining recipe and model
  poly_workflow <- workflow() %>%
    add_recipe(model_recipe) %>%
    add_model(lin_reg_spec)
  
  poly_fit <- poly_workflow %>%
    fit(data = data)
  
  model_fit <- poly_fit %>% extract_fit_parsnip()
  model <- model_fit$fit
  
  predictions <- predict(poly_fit, new_data = data)
  rmse_val <- rmse(bind_cols(data, predictions), truth = salary, estimate = .pred)$.estimate
  
  r_squared <- glance(model)$r.squared
  adj_r_squared <- glance(model)$adj.r.squared
  
  # Return results
  return(list(
    workflow = poly_fit,
    model = model,
    summary = summary(model),
    recipe = model_recipe,
    metrics = tibble(
      gpa_degree = max_gpa_degree,
      exp_degree = max_exp_degree,
      rmse = rmse_val,
      r_squared = r_squared,
      adj_r_squared = adj_r_squared
    )
  ))
}

# Evaluate models with degrees 1 to 3
poly_results_tidy <- map(1:3, evaluate_poly_model_tidy)

poly_metrics_tidy <- map_dfr(poly_results_tidy, ~ .x$metrics)
print(poly_metrics_tidy)

# Find best model
best_model_idx <- which.max(poly_metrics_tidy$adj_r_squared)
best_poly_workflow <- poly_results_tidy[[best_model_idx]]$workflow
best_poly_model <- poly_results_tidy[[best_model_idx]]$model

cat("Best polynomial model:\n")
print(poly_results_tidy[[best_model_idx]]$recipe)
cat("\n")
tidy(best_poly_model)
glance(best_poly_model)

# Plot relationship using degree from best model
best_gpa_degree <- poly_metrics_tidy$gpa_degree[best_model_idx]

# Create plot
ggplot(placed_grads, aes(x = gpa, y = salary)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", 
              formula = paste0("y ~ poly(x, ", best_gpa_degree, ")"), 
              color = "blue") +
  labs(title = paste0("Polynomial relationship (degree ", best_gpa_degree, ") between GPA and Salary"),
       x = "GPA",
       y = "Salary ($)") +
  theme_minimal()

base_recipe <- recipe(salary ~ gpa + years_of_experience + gender_Male + 
                      stream_Computer.Science + stream_Electrical.Engineering + 
                      stream_Electronics.and.Communication + stream_Information.Technology, 
                      data = placed_grads)

base_workflow <- workflow() %>%
  add_recipe(base_recipe) %>%
  add_model(lin_reg_spec) %>%
  fit(data = placed_grads)

base_model_tidy <- extract_fit_parsnip(base_workflow)$fit

#anova(base_model_tidy, best_poly_model)
```

The polynomial regression analysis reveals strong evidence for nonlinear relationships between academic performance and starting salary outcomes. Comparing model performance across different polynomial degrees demonstrates that a quadratic relationship (degree 2) substantially outperforms the linear model, with R-squared increasing from 0.369 to 0.468—indicating that nonlinear terms explain approximately 10% more variance in salary outcomes. This improvement is also supported by a significant reduction in prediction error, with RMSE decreasing from 1785.31 to 1638.73. The negligible improvement when advancing to a cubic model (R-squared of 0.470 versus 0.468) suggests that a quadratic function adequately captures the relationship's nonlinearity, with the adjusted R-squared values confirming these improvements aren't merely artifacts of additional parameters. These findings strongly indicate that the GPA-salary relationship follows a curvilinear pattern, where the marginal returns of increasing GPA may vary at different performance levels.


```{r}
tidy(best_poly_model)
```
The first-degree GPA term `(poly(gpa, 3)1)` shows a strong positive effect (44065.82, p<0.001), confirming that higher academic performance significantly increases starting salary. Interestingly, while the second-degree GPA term is not statistically significant, the third-degree term approaches significance, suggesting complex nonlinear effects at the extremes of the GPA range. `Years of experience` demonstrates powerful nonlinear effects, with both first-degree (27071.23, p<0.001) and second-degree (25024.36, p<0.001) terms being highly significant, indicating diminishing returns with increasing experience. Gender shows no significant salary difference (p=0.912). Academic streams (Computer Science and Electrical Engineering) show no significant differences from the reference category. However, college tier is highly significant, with Tier L institutions associated with substantially lower starting salaries (-2132.43, p<0.001) compared to the reference tier. These findings shows that salary outcomes are shaped by a complex interplay of academic performance, experience, and institutional prestige, with nonlinear effects particularly evident in the experience variable.


